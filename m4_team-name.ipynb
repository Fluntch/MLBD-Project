{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8af48ca1-b6d1-4092-b7b5-037d3c2d7aef",
   "metadata": {
    "id": "8af48ca1-b6d1-4092-b7b5-037d3c2d7aef"
   },
   "source": [
    "# M4 | Research Investigation Notebook\n",
    "\n",
    "In this notebook, you will do a research investigation of your chosen dataset in teams. You will begin by formally selecting your research question (task 0), then processing your data (task 1), creating a predictive model (task 2), evaluating your model's results (task 3), and describing the contributions of each team member (task 4).\n",
    "\n",
    "For grading, please make sure your notebook has all cells already run. You will also need to write a short, 2 page report about your design decisions as a team, to be uploaded to Moodle in the form of a PDF file next to this Jupyter notebook.\n",
    "\n",
    "You should provide arguments and justifications for all of your design decisions throughout this investigation. You can use your M3 responses as the basis for this discussion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ea2d32-f0a9-4dc9-bb60-be43399f5b89",
   "metadata": {
    "id": "82ea2d32-f0a9-4dc9-bb60-be43399f5b89"
   },
   "outputs": [],
   "source": [
    "# Import the tables of the data set as dataframes.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "DATA_DIR = './data' # You many change the directory\n",
    "\n",
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89137355",
   "metadata": {},
   "source": [
    "## Task 0: Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dafc5b",
   "metadata": {},
   "source": [
    "**Research question:**\n",
    "\n",
    "*Predict users performance based on their activity behaviour*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a77f62b0-1945-48f1-8f22-5f6ebda1db8e",
   "metadata": {
    "id": "a77f62b0-1945-48f1-8f22-5f6ebda1db8e"
   },
   "source": [
    "## Task 1: Data Preprocessing\n",
    "\n",
    "In this section, you are asked to preprocess your data in a way that is relevant for the model. Please include 1-2 visualizations of features / data explorations that are related to your downstream prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cefd0d-4f8a-4227-8fb4-f30521abf78d",
   "metadata": {
    "id": "34cefd0d-4f8a-4227-8fb4-f30521abf78d"
   },
   "outputs": [],
   "source": [
    "# Your code for data processing goes here\n",
    "\n",
    "### Here call your scripts for preprocessing, also explain the preprocessing steps that you did in general + data loss + graphs \n",
    "\n",
    "activity = pd.read_csv('{}/features/activity.csv'.format(DATA_DIR))\n",
    "performances = pd.read_csv('{}/features/performances.csv'.format(DATA_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** Unfortunately, many of the activity data points are not useable as our main method relies on looking at the behaviour of the students and therefore the time spent on activities is important to us. In the following graph we can see the ammount of activity data that is of length 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = activity.groupby('domain')['user_id'].count()\n",
    "unuseable = activity[activity['activity_completed'].isna()]\n",
    "unuseable_count = unuseable.groupby('domain')['user_id'].count()\n",
    "\n",
    "domains = sorted(set(total.index).union(set(unuseable_count.index)))\n",
    "total = total.reindex(domains, fill_value=0)\n",
    "unuseable_count = unuseable_count.reindex(domains, fill_value=0)\n",
    "\n",
    "x = np.arange(len(domains))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(x - width/2, total, width, label='Total')\n",
    "plt.bar(x + width/2, unuseable_count, width, label='Unusable')\n",
    "\n",
    "plt.ylabel('Number of Users')\n",
    "plt.xlabel('Domain')\n",
    "plt.title('Total vs Unusable Activity data by Domain')\n",
    "plt.xticks(x, domains, rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'By deleting these colums, we lose {(unuseable_count.sum()/total.sum() * 100).round(2)} % of the initial data ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** This is quite a big loss of data but a necessary one... (see annexe for trial without this loss of data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering \n",
    "\n",
    "**Our aim is to predict a user's score on their first attempt at a specific exam, based on their behavior leading up to it. To train our model, we engineered the following features to capture and characterize user behavior:**\n",
    "\n",
    "- recent_avg_time_per_activity\n",
    "\n",
    "*The average time (in minutes) spent per activity during the last 10 days before the exam, including the exam day. This reflects how intensely the user has been engaging recently.*\n",
    "\n",
    "- days_since_last_activity\n",
    "\n",
    "*The number of days between the user's most recent activity and the exam. A higher value may indicate a longer gap in studying before the exam.*\n",
    "\n",
    "- total_time_spent_on_activity_before_exam\n",
    "\n",
    "*The total amount of time (in minutes) the user spent on activities prior to the exam. This gives a sense of overall study investment leading up to the test.*\n",
    "\n",
    "- average_performance_past_exams\n",
    "\n",
    "*The mean performance from the user's previous exams (before this one). This can provide a rough estimate of the user's historical performance.*\n",
    "\n",
    "- avg_activities_per_day_recent\n",
    "\n",
    "*The average number of activities completed per day in the 10-day rolling window. A higher number may suggest more consistent or intense preparation.*\n",
    "\n",
    "- active_days_ratio_recent\n",
    "\n",
    "*The proportion of days (out of 10) on which the user was active. This indicates how regularly they studied leading up to the exam.*\n",
    "\n",
    "- diversity_recent\n",
    "\n",
    "*The number of unique activity types completed in the rolling window. Higher diversity could reflect more varied engagement with different learning methods or content types.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling window for recent activity\n",
    "rolling_window_days = 10\n",
    "\n",
    "\n",
    "def compute_all_features_for_exam(exam_row, user_activities, user_exams, window_days=rolling_window_days):\n",
    "\n",
    "    exam_dt = exam_row['time']\n",
    "\n",
    "\n",
    "    # Include activities up to and including exam_date\n",
    "    previous_activities = user_activities[user_activities['activity_updated'] < exam_dt].copy()\n",
    "\n",
    "\n",
    "    # Rolling window (activities in the last N days, including exam day)\n",
    "    window_start = exam_dt - pd.Timedelta(days=window_days)\n",
    "    rolling_activities = previous_activities[previous_activities['activity_updated'] >= window_start].copy()\n",
    "\n",
    "    features = {}\n",
    "\n",
    "\n",
    "    # Recent average time per activity (rolling window)\n",
    "    total_time_rolling = rolling_activities['time_in_minutes'].sum()\n",
    "    count_rolling = len(rolling_activities)\n",
    "    features['recent_avg_time_per_activity'] = total_time_rolling / count_rolling if count_rolling > 0 else 0\n",
    "\n",
    "\n",
    "    # Number of days since last activity\n",
    "    if not previous_activities.empty:\n",
    "        last_activity_date = previous_activities['activity_updated'].max()\n",
    "        features['days_since_last_activity'] = (exam_dt - last_activity_date).days\n",
    "    else:\n",
    "        features['days_since_last_activity'] = np.nan\n",
    "\n",
    "\n",
    "    # Total time spent on activities before the exam\n",
    "    features['total_time_spent_on_activity_before_exam'] = previous_activities['time_in_minutes'].sum() if not previous_activities.empty else 0\n",
    "\n",
    "\n",
    "    # Average performance on past exams\n",
    "    previous_exams = user_exams[user_exams['time'] < exam_dt]\n",
    "    features['average_performance_past_exams'] = previous_exams['performance'].mean() if not previous_exams.empty else np.nan\n",
    "\n",
    "\n",
    "    # Usage Frequency: Average activities per day in rolling window & Active days ratio\n",
    "    features['avg_activities_per_day_recent'] = count_rolling / window_days if window_days > 0 else np.nan\n",
    "    if not rolling_activities.empty:\n",
    "        distinct_days = rolling_activities['activity_updated'].dt.normalize().nunique()\n",
    "    else:\n",
    "        distinct_days = 0\n",
    "    features['active_days_ratio_recent'] = distinct_days / window_days if window_days > 0 else np.nan\n",
    "\n",
    "\n",
    "    # Activity diversity (rolling window)\n",
    "    features['diversity_recent'] = rolling_activities['activity_type'].nunique() if not rolling_activities.empty else 0\n",
    "\n",
    "\n",
    "    return pd.Series(features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All features are calculated separately within each domain — Math, Text, and Essay — to capture domain-specific learning patterns and engagement.**\n",
    "\n",
    "We have also scaled the features using a standard because we assume a normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "columns_to_scale = ['recent_avg_time_per_activity', 'days_since_last_activity', 'total_time_spent_on_activity_before_exam','average_performance_past_exams','avg_activities_per_day_recent','diversity_recent']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Math data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performances_math = performances[performances['domain']== 'math'].copy()\n",
    "activity_math = activity[activity['domain']== 'math'].copy()\n",
    "\n",
    "# Convert the 'date' columns to datetime\n",
    "activity_math['activity_updated'] = pd.to_datetime(activity_math['activity_updated'])\n",
    "performances_math['time'] = pd.to_datetime(performances_math['time'])\n",
    "\n",
    "# Loop over each exam (grouped by user) in performances_math and compute all features.\n",
    "features_list = []\n",
    "\n",
    "for user_id, user_exams in performances_math.groupby('user_id'):\n",
    "    # Get corresponding activities for the user from activity_math and sort by date\n",
    "    user_activities = activity_math[activity_math['user_id'] == user_id].sort_values('activity_updated')\n",
    "    user_exams_sorted = user_exams.sort_values('time')\n",
    "\n",
    "    for exam_index, exam_row in user_exams_sorted.iterrows():\n",
    "        feats = compute_all_features_for_exam(exam_row, user_activities, user_exams_sorted, rolling_window_days)\n",
    "        feats['exam_index'] = exam_index\n",
    "        features_list.append(feats)\n",
    "\n",
    "# Output df\n",
    "features_df = pd.DataFrame(features_list).set_index('exam_index')\n",
    "performances_math_features = performances_math.join(features_df, how='left')\n",
    "\n",
    "\n",
    "# scaling the columns\n",
    "scaled_values = scaler.fit_transform(performances_math_features[columns_to_scale])\n",
    "scaled_df = pd.DataFrame(scaled_values, columns=columns_to_scale, index=performances_math_features.index)\n",
    "remaining_df = performances_math_features.drop(columns=columns_to_scale)\n",
    "final_df_math = pd.concat([scaled_df, remaining_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_math.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Nb of rows :', final_df_math.user_id.count())\n",
    "print('Nb of rows where we are missing feature values :', final_df_math.isna().any(axis=1).sum())\n",
    "print('Percentage of rows where we are missing feature values :', final_df_math.isna().any(axis=1).sum()/ final_df_math.user_id.count() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** Some features for certain rows cannot be computed, particularly the \"average_performance_past_exams\" feature. This occurs for the first exam questions attempted by each user, where no prior performance data is available.\n",
    "\n",
    "**Comment:** For the math dataset, approximately 12% of the data is affected by this issue. However, if we were to remove this data the loss is minor and there are still around 3400 healthy data points that remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\n",
    "    'recent_avg_time_per_activity',\n",
    "    'days_since_last_activity',\n",
    "    'total_time_spent_on_activity_before_exam',\n",
    "    'average_performance_past_exams',\n",
    "    'avg_activities_per_day_recent',\n",
    "    'diversity_recent',\n",
    "    'active_days_ratio_recent',\n",
    "    'performance'  \n",
    "]\n",
    "\n",
    "correlation_matrix_math = final_df_math[feature_columns].corr()\n",
    "\n",
    "plt.figure(figsize=(8, 7))\n",
    "sns.heatmap(correlation_matrix_math, annot=True, fmt=\".2f\", cmap='coolwarm', center=0)\n",
    "plt.title(\"Correlation Matrix of Features (Math Domain)\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Essay data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performances_essay = performances[performances['domain']== 'essay'].copy()\n",
    "activity_essay= activity[activity['domain']== 'essay'].copy()\n",
    "\n",
    "# Convert the date columns  to datetime\n",
    "activity_essay['activity_updated'] = pd.to_datetime(activity_essay['activity_updated'])\n",
    "performances_essay['time'] = pd.to_datetime(performances_essay['time'])\n",
    "\n",
    "# Loop over each exam (grouped by user) in performances_essay and compute all features.\n",
    "features_list = []\n",
    "\n",
    "for user_id, user_exams in performances_essay.groupby('user_id'):\n",
    "    # Get corresponding activities for the user from activity_essay and sort by date\n",
    "    user_activities = activity_essay[activity_essay['user_id'] == user_id].sort_values('date')\n",
    "    user_exams_sorted = user_exams.sort_values('date')\n",
    "\n",
    "    for exam_index, exam_row in user_exams_sorted.iterrows():\n",
    "        feats = compute_all_features_for_exam(exam_row, user_activities, user_exams_sorted, rolling_window_days)\n",
    "        feats['exam_index'] = exam_index\n",
    "        features_list.append(feats)\n",
    "\n",
    "# Output df\n",
    "features_df = pd.DataFrame(features_list).set_index('exam_index')\n",
    "performances_essay_features = performances_essay.join(features_df, how='left')\n",
    "\n",
    "\n",
    "# scaling the columns\n",
    "scaled_values = scaler.fit_transform(performances_essay_features[columns_to_scale])\n",
    "scaled_df = pd.DataFrame(scaled_values, columns=columns_to_scale, index=performances_essay_features.index)\n",
    "remaining_df = performances_essay_features.drop(columns=columns_to_scale)\n",
    "final_df_essay = pd.concat([scaled_df, remaining_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_essay.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Nb of rows :', final_df_essay.user_id.count())\n",
    "print('Nb of rows where we are missing feature values :', final_df_essay.isna().any(axis=1).sum())\n",
    "print('Percentage of rows where we are missing feature values :', final_df_essay.isna().any(axis=1).sum()/ final_df_essay.user_id.count() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** The essay domain has very limited data available, with only 581 rows usable for training. \n",
    "\n",
    "**Comment:** Many rows are also missing the \"average_performance_past_exams\" feature. These are missing for the same reason as previously mentionned. Due to the already limited amount of data, this will prove to be problematic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix_essay = final_df_essay[feature_columns].corr()\n",
    "\n",
    "plt.figure(figsize=(8, 7))\n",
    "sns.heatmap(correlation_matrix_essay, annot=True, fmt=\".2f\", cmap='coolwarm', center=0)\n",
    "plt.title(\"Correlation Matrix of Features (Essay Domain)\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performances_text = performances[performances['domain']== 'text'].copy()\n",
    "activity_text= activity[activity['domain']== 'text'].copy()\n",
    "\n",
    "# Convert the date columns to datetime\n",
    "activity_text['activity_updated'] = pd.to_datetime(activity_text['activity_updated'])\n",
    "performances_text['time'] = pd.to_datetime(performances_text['time'])\n",
    "\n",
    "# Loop over each exam (grouped by user) in performances_text and compute all features.\n",
    "features_list = []\n",
    "\n",
    "for user_id, user_exams in performances_text.groupby('user_id'):\n",
    "    # Get corresponding activities for the user from activity_text and sort by date\n",
    "    user_activities = activity_text[activity_text['user_id'] == user_id].sort_values('date')\n",
    "    user_exams_sorted = user_exams.sort_values('date')\n",
    "\n",
    "    for exam_index, exam_row in user_exams_sorted.iterrows():\n",
    "        feats = compute_all_features_for_exam(exam_row, user_activities, user_exams_sorted, rolling_window_days)\n",
    "        feats['exam_index'] = exam_index\n",
    "        features_list.append(feats)\n",
    "\n",
    "# Output df\n",
    "features_df = pd.DataFrame(features_list).set_index('exam_index')\n",
    "performances_text_features = performances_text.join(features_df, how='left')\n",
    "\n",
    "\n",
    "# Scale the columns\n",
    "scaled_values = scaler.fit_transform(performances_text_features[columns_to_scale])\n",
    "scaled_df = pd.DataFrame(scaled_values, columns=columns_to_scale, index=performances_text_features.index)\n",
    "remaining_df = performances_text_features.drop(columns=columns_to_scale)\n",
    "final_df_text = pd.concat([scaled_df, remaining_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_text.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Nb of rows :', final_df_text.user_id.count())\n",
    "print('Nb of rows where we are missing feature values :', final_df_text.isna().any(axis=1).sum())\n",
    "print('Percentage of rows where we are missing feature values :', final_df_text.isna().any(axis=1).sum()/ final_df_text.user_id.count() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** The text domain has even less data available, with only 445 rows usable for training. \n",
    "\n",
    "**Comment:** Approximately 80% of the rows are missing with most coming from \"days_since_last_activity\". This means that users took the exam with no prior activity logs, meaning that, for our research question, these rows are useless since we are trying to predict performance based on past activity. There are also many rows where the \"average_performance_past_exams\" feature is missing, which will prove problematic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix_text = final_df_essay[feature_columns].corr()\n",
    "\n",
    "plt.figure(figsize=(8, 7))\n",
    "sns.heatmap(correlation_matrix_text, annot=True, fmt=\".2f\", cmap='coolwarm', center=0)\n",
    "plt.title(\"Correlation Matrix of Features (Text Domain)\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d0fcca-686e-47a3-8567-c3f92db187d4",
   "metadata": {
    "id": "76d0fcca-686e-47a3-8567-c3f92db187d4"
   },
   "source": [
    "*Your discussion about your processing decisions goes here*\n",
    "\n",
    "\n",
    "When looking at the correlation analysis, we observe that the feature most strongly correlated with performance is \"average_performance_past_exams.\" This presents a significant problem for the essay and text datasets, where not only are there very few rows to begin with, but this important feature is often missing. As a result, we decided to focus our efforts on the math data, where we have a much healthier number of data points available.\n",
    "\n",
    "We did attempt to train models on the essay and text data; however, the performance was very poor, likely due to the limited amount of training examples and the missing key features. Training a model with so little data is highly unreliable and unlikely to generalize well. On the other hand, if our approach works well on the math data, it suggests that the methodology could be transferable to other domains, provided enough data points are available to fit a model properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85633adb-d317-4ee3-bf06-e9f82f589c41",
   "metadata": {
    "id": "85633adb-d317-4ee3-bf06-e9f82f589c41"
   },
   "source": [
    "## Task 2: Model Building\n",
    "\n",
    "Train a model for your research question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** Seeing the lack of data available to us for both the text and essay domains, we decided to concentrate on the math data for the following parts. We also tried out multiple models to try and see if we could improve performance at all. Therefore all following models are solely for the math data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 0 : Simple Linear Regression where we drop NAN\n",
    "\n",
    "Sometimes, the simplest approaches can be the most effective. For our baseline model, we began by dropping any rows containing missing values (NaNs) to ensure a clean dataset. We then applied a simple linear regression model to predict exam performance based on the available features. This served as a straightforward benchmark to compare against more complex models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b65ebd-c148-4ae8-833e-018411eeda86",
   "metadata": {
    "id": "90b65ebd-c148-4ae8-833e-018411eeda86"
   },
   "outputs": [],
   "source": [
    "final_df_math_drop = final_df_math.copy()\n",
    "final_df_math_drop.dropna(inplace=True)\n",
    "\n",
    "\n",
    "# Linear Regression Model\n",
    "mod_method0 = smf.ols(formula='performance ~  recent_avg_time_per_activity + days_since_last_activity + total_time_spent_on_activity_before_exam + average_performance_past_exams + avg_activities_per_day_recent + diversity_recent', data=final_df_math_drop)\n",
    "# Fit the model\n",
    "res_method0 = mod_method0.fit()\n",
    "\n",
    "# Print regression results summary \n",
    "#print(res_method0.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1 : Mixed Effects Linear Regression where we drop NAN and cluster the users \n",
    "\n",
    "To enhance our simple linear regression model, we employed a K-Means clustering approach to group users based on their feature profiles. This allowed us to account for heterogeneity among users by capturing patterns in behavior and performance. By incorporating these clusters as mixed effects, we aimed to improve the model’s ability to generalize across diverse user groups and better explain variations in exam outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collapse to one row per user for clustering\n",
    "user_feats = final_df_math_drop.groupby('user_id')[['recent_avg_time_per_activity', 'days_since_last_activity', 'total_time_spent_on_activity_before_exam', 'average_performance_past_exams', 'avg_activities_per_day_recent', 'active_days_ratio_recent', 'diversity_recent']].mean().reset_index()\n",
    "\n",
    "# Cluster into 3 groups\n",
    "kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "user_feats['cluster'] = kmeans.fit_predict(user_feats.drop(columns='user_id'))\n",
    "\n",
    "# Merge cluster label back into final_df\n",
    "df2 = final_df_math_drop.merge(user_feats[['user_id','cluster']], on='user_id')\n",
    "\n",
    "# Fit a mixed‐effects model with random intercept per cluster\n",
    "mod_method1 = smf.mixedlm(\"performance ~ recent_avg_time_per_activity + days_since_last_activity + total_time_spent_on_activity_before_exam + average_performance_past_exams + avg_activities_per_day_recent + active_days_ratio_recent + diversity_recent\",df2,groups=\"cluster\")\n",
    "res_method1 = mod_method1.fit(reml=False)\n",
    "\n",
    "# In-sample predictions\n",
    "df2[\"pred_cluster\"] = res_method1.predict(df2)\n",
    "\n",
    "# Compute R² and RMSE\n",
    "# r2_cluster = r2_score(df2[\"performance\"], df2[\"pred_cluster\"])\n",
    "# rmse_cluster = mean_squared_error(df2[\"performance\"], df2[\"pred_cluster\"])\n",
    "\n",
    "# print(\"Cluster‐model  AIC:\",  mdf.aic, \"  BIC:\", mdf.bic)\n",
    "# print(f\"Cluster‐model  R² = {r2_cluster:.3f},  RMSE = {rmse_cluster:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2 : Simple Linear Regression where we impute the missing NAN values with the median value for that exam\n",
    "\n",
    "\n",
    "For this method we used a simple linear regression model with median imputation for missing values. Specifically, any NaN values were filled with the median value for the corresponding feature within each exam. This approach preserved more data compared to dropping rows and allowed the model to leverage a broader training set while maintaining simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_math_median = performances_math_features.copy()\n",
    "\n",
    "# Fill by test_id median\n",
    "performance_math_median['average_performance_past_exams'] = (\n",
    "    performance_math_median\n",
    "    .groupby('test_id')['average_performance_past_exams']\n",
    "    .transform(lambda x: x.fillna(x.median()))\n",
    "    .fillna(performance_math_median['average_performance_past_exams'].median())   # fallback to global median if a test has no median\n",
    ")\n",
    "\n",
    "# scaling the columns\n",
    "columns_to_scale = ['recent_avg_time_per_activity', 'days_since_last_activity', 'total_time_spent_on_activity_before_exam','average_performance_past_exams','avg_activities_per_day_recent','active_days_ratio_recent','diversity_recent']\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_values = scaler.fit_transform(performance_math_median[columns_to_scale])\n",
    "scaled_df = pd.DataFrame(scaled_values, columns=columns_to_scale, index=performance_math_median.index)\n",
    "remaining_df = performance_math_median.drop(columns=columns_to_scale)\n",
    "final_df_med = pd.concat([scaled_df, remaining_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Model\n",
    "mod_method2 = smf.ols(formula= 'performance ~  recent_avg_time_per_activity + days_since_last_activity + total_time_spent_on_activity_before_exam + average_performance_past_exams + avg_activities_per_day_recent + active_days_ratio_recent + diversity_recent', data=final_df_med)\n",
    "\n",
    "# Fit the model\n",
    "res_method2 = mod_method2.fit()\n",
    "\n",
    "# Print regression results summary\n",
    "#print(res_method2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3 : Simple Linear Regression where we impute the missing NAN values using KNN\n",
    "\n",
    "For Method 3, we applied a simple linear regression model with K-Nearest Neighbors (KNN) imputation for missing values. Instead of using global statistics like the median, KNN imputation estimates missing values based on the most similar observations in the dataset. This method allows for more context-aware imputations, potentially preserving underlying data relationships and improving the model’s predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_math_impute = performances_math_features.copy()\n",
    "\n",
    "# Features we’ll use to compute similarity:\n",
    "sim_features = ['recent_avg_time_per_activity','days_since_last_activity','total_time_spent_on_activity_before_exam',\n",
    "                'avg_activities_per_day_recent','active_days_ratio_recent','diversity_recent']\n",
    "\n",
    "# New column to hold the imputed values\n",
    "performance_math_impute['avg_perf_past_exams_imputed'] = performance_math_impute['average_performance_past_exams']\n",
    "\n",
    "# Group by test_id and run kNN inside each group\n",
    "for test_id, group in performance_math_impute.groupby('test_id'):\n",
    "    # indices of rows we need to fill\n",
    "    missing_idx = group[group['average_performance_past_exams'].isna()].index\n",
    "    if len(missing_idx) == 0:\n",
    "        continue\n",
    "\n",
    "    # candidate neighbors: same test, non‐missing avg_pct\n",
    "    candidates = group[group['average_performance_past_exams'].notna()]\n",
    "    if candidates.shape[0] == 0:\n",
    "        # if no one else took that test --> impute median\n",
    "        continue\n",
    "\n",
    "    # Matrix of sim_features, impute median for remaining NaNs\n",
    "    feat_mat = group[sim_features].copy()\n",
    "    feat_mat = feat_mat.fillna(feat_mat.median())\n",
    "\n",
    "    # Split into X_train (candidates) and X_query (the missing rows)\n",
    "    X_train = feat_mat.loc[candidates.index].values\n",
    "    X_query = feat_mat.loc[missing_idx].values\n",
    "\n",
    "    # Use up to 3 neighbors (fewer if not enough candidates)\n",
    "    k = min(3, X_train.shape[0])\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, algorithm='auto').fit(X_train)\n",
    "    distances, neighbors = nbrs.kneighbors(X_query)\n",
    "\n",
    "    # For each missing row, average the actual test scores of its neighbors\n",
    "    candidate_scores = candidates['performance'].values\n",
    "    for i, idx in enumerate(missing_idx):\n",
    "        nbr_idxs = neighbors[i]\n",
    "        imputed_val = candidate_scores[nbr_idxs].mean()\n",
    "        performance_math_impute.at[idx, 'avg_perf_past_exams_imputed'] = imputed_val\n",
    "\n",
    "# Replace the original column\n",
    "performance_math_impute['average_performance_past_exams'] = performance_math_impute['avg_perf_past_exams_imputed']\n",
    "performance_math_impute.drop(columns='avg_perf_past_exams_imputed', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the columns\n",
    "columns_to_scale = ['recent_avg_time_per_activity', 'days_since_last_activity', 'total_time_spent_on_activity_before_exam','average_performance_past_exams','avg_activities_per_day_recent','active_days_ratio_recent','diversity_recent']\n",
    "scaler = StandardScaler()\n",
    "scaled_values = scaler.fit_transform(performance_math_impute[columns_to_scale])\n",
    "scaled_df = pd.DataFrame(scaled_values, columns=columns_to_scale, index=performance_math_impute.index)\n",
    "remaining_df = performance_math_impute.drop(columns=columns_to_scale)\n",
    "final_df_impute = pd.concat([scaled_df, remaining_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_impute.dropna(inplace=True) # drop the remaining NaNs\n",
    "\n",
    "# Linear Regression Model\n",
    "mod_method3 = smf.ols(formula= 'performance ~  recent_avg_time_per_activity + days_since_last_activity + total_time_spent_on_activity_before_exam + average_performance_past_exams + avg_activities_per_day_recent + active_days_ratio_recent + diversity_recent', data=final_df_impute)\n",
    "\n",
    "# Fit the model\n",
    "res_method3 = mod_method3.fit()\n",
    "\n",
    "# Print regression results summary\n",
    "#print(res_method3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 4 : Gradient Boosting Regressor that accepts NAN values \n",
    "\n",
    "For Method 4, we used a Gradient Boosting Regressor that natively handles missing values. Unlike simpler models, gradient boosting can internally learn how to deal with NaNs during training, making it well-suited for real-world, imperfect data. This approach allowed us to retain the full dataset without requiring explicit imputation, while benefiting from the model's ability to capture complex, non-linear relationships in the features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = performances_math_features.copy()\n",
    "\n",
    "feature_cols = ['recent_avg_time_per_activity','days_since_last_activity','total_time_spent_on_activity_before_exam','average_performance_past_exams',\n",
    "    'avg_activities_per_day_recent','active_days_ratio_recent','diversity_recent']\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df['performance']\n",
    "\n",
    "# Train-Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Pipeline: impute → scale → gradient boost\n",
    "pipeline = Pipeline([\n",
    "    ('imputer',   SimpleImputer(strategy='median')),\n",
    "    ('scaler',    StandardScaler()),\n",
    "    ('gbr',       GradientBoostingRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Grid search for key hyperparameters\n",
    "param_grid = {'gbr__n_estimators': [100, 200], 'gbr__learning_rate': [0.05, 0.1], 'gbr__max_depth':[3, 5], 'gbr__subsample':[0.8, 1.0]}\n",
    "grid = GridSearchCV(pipeline, param_grid, cv=5, scoring='r2', n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit the model\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "# print(\"Test R²:\", r2_score(y_test, y_pred))\n",
    "# print(\"Test RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "# Cross‑validated performance\n",
    "# cv_scores = cross_val_score(best_model, X, y, cv=5, scoring='r2', n_jobs=-1)\n",
    "# print(\"5‑fold CV R²: %0.3f ± %0.3f\" % (cv_scores.mean(), cv_scores.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your discussion about your model training goes here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3c9a655-cec9-4c57-aec7-7a982f57a3af",
   "metadata": {
    "id": "b3c9a655-cec9-4c57-aec7-7a982f57a3af"
   },
   "source": [
    "## Task 3: Model Evaluation\n",
    "In this task, you will use metrics to evaluate your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 0 : \n",
    "\n",
    "Simple Linear Regression where we drop NAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print regression results summary\n",
    "print(res_method0.summary())\n",
    "\n",
    "\n",
    "final_df_math_drop['predicted_performance'] = res_method0.fittedvalues\n",
    "rmse_method0 = mean_squared_error(final_df_math_drop[\"performance\"], final_df_math_drop['predicted_performance'])\n",
    "print('RMSE :', rmse_method0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** The simple linear regression model where we drop the NAN's has an R-squared of 0.267, meaning it explains 26.7% of the variance in performance. While this is a relatively modest value, it's actually quite good for real world data, which tends to be noisy and influenced by factors not captured in the model. The significant predictors, such as \"recent_average_time_per_activity\" and \"total_time_spent_on_activity_before_exam\" suggest the model is capturing meaningful patterns. The \"average_performance_in_past_exams\" feature is also significant and has the largest coefficient. This reinforces our decision to focus on the math data, as this feature is less available in the essay and text domain data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Not convinced about this plot -----> keep it or not ?\n",
    "\n",
    "\n",
    "final_df_math_drop['predicted_performance'] = res_method0.fittedvalues\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "sns.scatterplot(x='average_performance_past_exams', y='performance', data=final_df_math_drop, color='orange', label='Actual Performance')\n",
    "sns.regplot(x='average_performance_past_exams', y='predicted_performance', data=final_df_math_drop, scatter_kws={'s': 10}, line_kws={'color': 'red'}, label='Predicted Performance')\n",
    "\n",
    "plt.xlabel('Average Performance in Past Exams')\n",
    "plt.ylabel('Performance')\n",
    "plt.title('Predicted vs. Actual Student Performance Based on Past Exam Averages')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** The plot illustrates the relationship between the \"average_performance_in_past_exams\" and the model’s predicted performance. This feature showed the strongest correlation with actual performance in the preliminary analysis and also emerged as the most significant predictor in our model, with the largest coefficient. Visualizing its impact helps us understand how well the model captures this relationship. As seen in the plot, there is a clear upward trend in the actual performance data, and the model has effectively captured this pattern in its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "sns.scatterplot(x='predicted_performance', y='performance', data=final_df_math_drop, color='dodgerblue')\n",
    "\n",
    "min_val = min(final_df_math_drop['performance'].min(), final_df_math_drop['predicted_performance'].min())\n",
    "max_val = max(final_df_math_drop['performance'].max(), final_df_math_drop['predicted_performance'].max())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], ls=\"--\", color=\"gray\", label='Perfect Prediction')\n",
    "\n",
    "buffer = 20\n",
    "plt.plot([min_val, max_val], [min_val + buffer, max_val + buffer], ls=\"--\", color=\"red\", alpha=0.5, label=f'+{buffer} Margin')\n",
    "plt.plot([min_val, max_val], [min_val - buffer, max_val - buffer], ls=\"--\", color=\"red\", alpha=0.5, label=f'-{buffer} Margin')\n",
    "\n",
    "mean_actual = 0\n",
    "mean_pred = 0\n",
    "\n",
    "plt.axhline(mean_actual, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "plt.axvline(mean_pred, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "\n",
    "plt.xlabel('Actual Performance')\n",
    "plt.ylabel('Predicted Performance')\n",
    "plt.title(\"Predicted vs Actual Performance per Cluster\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** By examining the model’s predictions against the actual performance values, we gain a better sense of its precision. The model captures the overall trend in performance reasonably well, though there is still a noticeable amount of error — which is to be expected given the complexity and variability of the data.\n",
    "\n",
    "That said, there are encouraging signs. The black dashed lines divide the plot into four quadrants that reflect the directional accuracy of the model’s predictions. A well-performing model should have most points in the top-right (both predicted and actual are high) and bottom-left (both predicted and actual are low) quadrants. In this case, the majority of points do fall into these two quadrants, suggesting that the model generally gets the direction of performance correct, even if it doesn’t always predict the exact value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions_model0 = [\n",
    "    (final_df_math_drop[\"predicted_performance\"] >= mean_pred) & (final_df_math_drop[\"performance\"] >= mean_actual),  # TP\n",
    "    (final_df_math_drop[\"predicted_performance\"] < mean_pred) & (final_df_math_drop[\"performance\"] < mean_actual),    # TN\n",
    "    (final_df_math_drop[\"predicted_performance\"] >= mean_pred) & (final_df_math_drop[\"performance\"] < mean_actual),   # FP\n",
    "    (final_df_math_drop[\"predicted_performance\"] < mean_pred) & (final_df_math_drop[\"performance\"] >= mean_actual),   # FN\n",
    "]\n",
    "\n",
    "choices = ['TP', 'TN', 'FP', 'FN']\n",
    "final_df_math_drop['quadrant'] = np.select(conditions_model0, choices, default='Other')\n",
    "\n",
    "counts_model0 = final_df_math_drop['quadrant'].value_counts()\n",
    "\n",
    "print(\"Quadrant Classification Counts:\")\n",
    "print(counts_model0)\n",
    "\n",
    "tp = counts_model0.get('TP', 0)\n",
    "tn = counts_model0.get('TN', 0)\n",
    "fp = counts_model0.get('FP', 0)\n",
    "fn = counts_model0.get('FN', 0)\n",
    "\n",
    "directional_accuracy_model0 = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(f\"\\nDirectional Accuracy: {directional_accuracy_model0:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** The directional accuracy of our model is 0.7030, meaning it correctly predicts whether a student’s performance will be above or below zero in 70% of cases. This indicates that the model is successfully learning meaningful patterns in user behavior that help it distinguish between generally good and poor performance. While it’s not perfect, this level of directional accuracy is an encouraging sign!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 :\n",
    "\n",
    "Mixed Effects Linear Regression where we drop NAN and cluster the users "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res_method1.summary())\n",
    "\n",
    "# Compute R² and RMSE\n",
    "r2_cluster = r2_score(df2[\"performance\"], df2[\"pred_cluster\"])\n",
    "rmse_cluster = mean_squared_error(df2[\"performance\"], df2[\"pred_cluster\"])\n",
    "\n",
    "print(\"Cluster‐model  AIC:\",  res_method1.aic, \"  BIC:\", res_method1.bic)\n",
    "print(f\"Cluster‐model  R² = {r2_cluster:.3f},  RMSE = {rmse_cluster:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** The mixed effects linear regression model has a R-squared value of 0.252. This is surprisingly lower than the simpler model 0, but not by much and it still seems to perform fairly well. The RMSE is also higher than model_0 which is another indicator of a worse fit to the data. For the significant features, \"average_performance_past_exams\" again seems to be the most prevelant feature, with other like \"avg_activities_per_day_recent\" and \"recent_avg_time_per_activity\" also contributing postiviely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = 20\n",
    "\n",
    "g = sns.FacetGrid(df2, col=\"cluster\", col_wrap=3, height=5, sharex=True, sharey=True)\n",
    "\n",
    "g.map_dataframe(sns.scatterplot, x=\"performance\", y=\"pred_cluster\", color='dodgerblue')\n",
    "\n",
    "min_val = min(df2[\"performance\"].min(), df2[\"pred_cluster\"].min())\n",
    "max_val = max(df2[\"performance\"].max(), df2[\"pred_cluster\"].max())\n",
    "\n",
    "for i, ax in enumerate(g.axes.flatten()):\n",
    "    \n",
    "    ax.plot([min_val, max_val], [min_val, max_val], ls=\"--\", color=\"gray\",\n",
    "            label=\"Perfect Prediction\" if i == 0 else \"\")\n",
    "    \n",
    "    ax.plot([min_val, max_val], [min_val + buffer, max_val + buffer], ls=\"--\", color=\"red\", alpha=0.5,\n",
    "            label=f'+{buffer} Margin' if i == 0 else \"\")\n",
    "    ax.plot([min_val, max_val], [min_val - buffer, max_val - buffer], ls=\"--\", color=\"red\", alpha=0.5,\n",
    "            label=f'-{buffer} Margin' if i == 0 else \"\")\n",
    "    \n",
    "    mean_actual = 0\n",
    "    mean_pred = 0\n",
    "\n",
    "    ax.axhline(mean_actual, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "    ax.axvline(mean_pred, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "\n",
    "g.axes[0].legend()\n",
    "\n",
    "g.set_axis_labels(\"Actual Performance\", \"Predicted Performance\")\n",
    "g.set_titles(\"Cluster {col_name}\")\n",
    "g.fig.suptitle(\"Predicted vs Actual Performance per Cluster\", fontsize=16, y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions_model1 = [\n",
    "    (df2[\"pred_cluster\"] >= mean_pred) & (df2[\"performance\"] >= mean_actual),  # TP\n",
    "    (df2[\"pred_cluster\"] < mean_pred) & (df2[\"performance\"] < mean_actual),    # TN\n",
    "    (df2[\"pred_cluster\"] >= mean_pred) & (df2[\"performance\"] < mean_actual),   # FP\n",
    "    (df2[\"pred_cluster\"] < mean_pred) & (df2[\"performance\"] >= mean_actual),   # FN\n",
    "]\n",
    "\n",
    "choices = ['TP', 'TN', 'FP', 'FN']\n",
    "df2['quadrant'] = np.select(conditions_model1, choices, default='Other')\n",
    "\n",
    "counts_model1 =df2['quadrant'].value_counts()\n",
    "\n",
    "print(\"Quadrant Classification Counts:\")\n",
    "print(counts_model1)\n",
    "\n",
    "tp = counts_model1.get('TP', 0)\n",
    "tn = counts_model1.get('TN', 0)\n",
    "fp = counts_model1.get('FP', 0)\n",
    "fn = counts_model1.get('FN', 0)\n",
    "\n",
    "directional_accuracy_model1 = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(f\"\\nDirectional Accuracy: {directional_accuracy_model1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** When examining the predicted vs. actual performance plots by cluster, we can see that the clustering appears to have been effective in grouping students with similar characteristics. Clusters 1 and 2 contain the majority of students, with Cluster 1 tending to include those with lower performance levels, while Cluster 2 generally includes higher-performing students.\n",
    "\n",
    "Although the model does not perfectly capture the variation in performance within each cluster, evident from the spread around the perfect prediction line, it still demonstrates meaningful signal. In particular, the directional accuracy remains around 70%, indicating that the model is reliably predicting whether a student will perform above or below the average. This is again a promising sign that the model is capturing relevant behavioral patterns, even if the exact predictions aren’t always precise. However, it is slightly lower than our simpler model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 :\n",
    "\n",
    "Simple Linear Regression where we impute the missing NAN values with the median value for that exam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(final_df_med[final_df_med[['performance', 'predicted_performance']].isna().any(axis=1)].isnull().sum())\n",
    "\n",
    "# final_df_med.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_med[final_df_med[['performance', 'predicted_performance']].isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res_method2.summary())\n",
    "\n",
    "final_df_med['predicted_performance'] = res_method2.fittedvalues\n",
    "rmse_method2 = mean_squared_error(final_df_med[\"performance\"], final_df_med['predicted_performance'])\n",
    "print('RMSE :', rmse_method2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** The simple linear regression model with imputed NAN's has a R-squared value of 0.242. This is notably lower than both of the previous models. Among the significant predictors, \"average_performance_past_exams\" remains the most influential, followed by \"recent_avg_time_per_activity\" and \"total_time_spent_on_activity_before_exam\", all of which contribute positively to the model's predictive capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_med['predicted_performance'] = res_method2.fittedvalues\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "sns.scatterplot(x='predicted_performance', y='performance', data=final_df_med, color='dodgerblue')\n",
    "\n",
    "min_val = min(final_df_math_drop['performance'].min(), final_df_math_drop['predicted_performance'].min())\n",
    "max_val = max(final_df_math_drop['performance'].max(), final_df_math_drop['predicted_performance'].max())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], ls=\"--\", color=\"gray\")\n",
    "\n",
    "buffer = 20\n",
    "plt.plot([min_val, max_val], [min_val + buffer, max_val + buffer], ls=\"--\", color=\"red\", alpha=0.5, label=f'+{buffer} Margin')\n",
    "plt.plot([min_val, max_val], [min_val - buffer, max_val - buffer], ls=\"--\", color=\"red\", alpha=0.5, label=f'-{buffer} Margin')\n",
    "\n",
    "mean_actual = 0\n",
    "mean_pred = 0\n",
    "\n",
    "plt.axhline(mean_actual, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "plt.axvline(mean_pred, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "\n",
    "plt.xlabel('Actual Performance')\n",
    "plt.ylabel('Predicted Performance')\n",
    "plt.title(\"Predicted vs Actual Performance per Cluster\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conditions_model2 = [\n",
    "    (final_df_med[\"predicted_performance\"] >= mean_pred) & (final_df_med[\"performance\"] >= mean_actual),  # TP\n",
    "    (final_df_med[\"predicted_performance\"] < mean_pred) & (final_df_med[\"performance\"] < mean_actual),    # TN\n",
    "    (final_df_med[\"predicted_performance\"] >= mean_pred) & (final_df_med[\"performance\"] < mean_actual),   # FP\n",
    "    (final_df_med[\"predicted_performance\"] < mean_pred) & (final_df_med[\"performance\"] >= mean_actual),   # FN\n",
    "]\n",
    "\n",
    "choices = ['TP', 'TN', 'FP', 'FN']\n",
    "final_df_med['quadrant'] = np.select(conditions_model2, choices, default='Other')\n",
    "\n",
    "\n",
    "counts_model2 = final_df_med['quadrant'].value_counts()\n",
    "print(\"Quadrant Classification Counts:\")\n",
    "print(counts_model2)\n",
    "\n",
    "\n",
    "tp = counts_model2.get('TP', 0)\n",
    "tn = counts_model2.get('TN', 0)\n",
    "fp = counts_model2.get('FP', 0)\n",
    "fn = counts_model2.get('FN', 0)\n",
    "\n",
    "directional_accuracy_model2 = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(f\"\\nDirectional Accuracy: {directional_accuracy_model2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 :\n",
    "\n",
    "Simple Linear Regression where we impute the missing NAN values using KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res_method3.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4 :\n",
    "\n",
    "Gradient Boosting Regressor that accepts NAN values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test R²:\", r2_score(y_test, y_pred))\n",
    "print(\"Test RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "cv_scores = cross_val_score(best_model, X, y, cv=5, scoring='r2', n_jobs=-1)\n",
    "print(\"5‑fold CV R²: %0.3f ± %0.3f\" % (cv_scores.mean(), cv_scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparisons between the models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here compare the results with maybe a visualisation and some analysis ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_method0.rsquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62968daa",
   "metadata": {},
   "source": [
    "*Your discussion/interpretation about your model's behavior goes here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e49d1dad",
   "metadata": {},
   "source": [
    "## Task 4: Team Reflection\n",
    "Please describe the contributions of each team member to Milestone 4. Reflect on how you worked as team: what went well, what can be improved for the next milestone?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cde86a72",
   "metadata": {},
   "source": [
    "*Your discussion about team responsibilities goes here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annexe\n",
    "\n",
    "Test without the time dependant activity features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity = pd.read_csv('{}/features/activity.csv'.format(DATA_DIR))\n",
    "performances = pd.read_csv('{}/features/performances.csv'.format(DATA_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performances_math = performances[performances['domain']== 'math'].copy()\n",
    "activity_math = activity[activity['domain']== 'math'].copy()\n",
    "\n",
    "# Rolling window for recent activity\n",
    "rolling_window_days = 10\n",
    "\n",
    "# Convert the 'date' columns to datetime\n",
    "activity_math['activity_updated'] = pd.to_datetime(activity_math['activity_updated'])\n",
    "performances_math['time'] = pd.to_datetime(performances_math['time'])\n",
    "\n",
    "def compute_all_features_for_exam_no_time(exam_row, user_activities, user_exams, window_days=rolling_window_days):\n",
    "\n",
    "    exam_dt = exam_row['time']\n",
    "\n",
    "    # Include activities up to and including exam_date\n",
    "    previous_activities = user_activities[user_activities['activity_updated'] < exam_dt].copy()\n",
    "\n",
    "    # Rolling window (activities in the last N days, including exam day)\n",
    "    window_start = exam_dt - pd.Timedelta(days=window_days)\n",
    "    rolling_activities = previous_activities[previous_activities['activity_updated'] >= window_start].copy()\n",
    "\n",
    "    features = {}\n",
    "\n",
    "    # Recent average time per activity (rolling window)\n",
    "    #total_time_rolling = rolling_activities['time_in_minutes'].sum()\n",
    "    count_rolling = len(rolling_activities)\n",
    "    #features['recent_avg_time_per_activity'] = total_time_rolling / count_rolling if count_rolling > 0 else 0\n",
    "\n",
    "    # Number of days since last activity\n",
    "    if not previous_activities.empty:\n",
    "        last_activity_date = previous_activities['activity_updated'].max()\n",
    "        features['days_since_last_activity'] = (exam_dt - last_activity_date).days\n",
    "    else:\n",
    "        features['days_since_last_activity'] = np.nan\n",
    "\n",
    "    # Total time spent on activities before the exam\n",
    "    #features['total_time_spent_on_activity_before_exam'] = previous_activities['time_in_minutes'].sum() if not previous_activities.empty else 0\n",
    "\n",
    "    # Average percentage on past exams\n",
    "    previous_exams = user_exams[user_exams['time'] < exam_dt]\n",
    "    features['average_performance_past_exams'] = previous_exams['performance'].mean() if not previous_exams.empty else np.nan\n",
    "\n",
    "    # Usage Frequency: Average activities per day in rolling window & Active days ratio\n",
    "    features['avg_activities_per_day_recent'] = count_rolling / window_days if window_days > 0 else np.nan\n",
    "    if not rolling_activities.empty:\n",
    "        distinct_days = rolling_activities['activity_updated'].dt.normalize().nunique()\n",
    "    else:\n",
    "        distinct_days = 0\n",
    "    features['active_days_ratio_recent'] = distinct_days / window_days if window_days > 0 else np.nan\n",
    "\n",
    "    # Activity diversity (rolling window)\n",
    "    features['diversity_recent'] = rolling_activities['activity_type'].nunique() if not rolling_activities.empty else 0\n",
    "\n",
    "\n",
    "    return pd.Series(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over each exam (grouped by user) in performances_math and compute all features.\n",
    "features_list = []\n",
    "\n",
    "for user_id, user_exams in performances_math.groupby('user_id'):\n",
    "    # Get corresponding activities for the user from activity_math and sort by date\n",
    "    user_activities = activity_math[activity_math['user_id'] == user_id].sort_values('activity_updated')\n",
    "    user_exams_sorted = user_exams.sort_values('time')\n",
    "\n",
    "    for exam_index, exam_row in user_exams_sorted.iterrows():\n",
    "        feats = compute_all_features_for_exam_no_time(exam_row, user_activities, user_exams_sorted, rolling_window_days)\n",
    "        feats['exam_index'] = exam_index\n",
    "        features_list.append(feats)\n",
    "\n",
    "# Output df\n",
    "features_df = pd.DataFrame(features_list).set_index('exam_index')\n",
    "performances_math_features = performances_math.join(features_df, how='left')\n",
    "\n",
    "\n",
    "# scaling the columns\n",
    "columns_to_scale = [ 'days_since_last_activity','average_performance_past_exams','avg_activities_per_day_recent','active_days_ratio_recent','diversity_recent']\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_values = scaler.fit_transform(performances_math_features[columns_to_scale])\n",
    "scaled_df = pd.DataFrame(scaled_values, columns=columns_to_scale, index=performances_math_features.index)\n",
    "remaining_df = performances_math_features.drop(columns=columns_to_scale)\n",
    "final_df = pd.concat([scaled_df, remaining_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Model\n",
    "mod_annexe = smf.ols(\n",
    "    formula='performance ~  days_since_last_activity + average_performance_past_exams + avg_activities_per_day_recent + active_days_ratio_recent + diversity_recent',\n",
    "    data=final_df)\n",
    "\n",
    "# Fit the model\n",
    "res_annexe = mod_annexe.fit()\n",
    "\n",
    "# Print regression results summary\n",
    "print(res_annexe.summary())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "m2-classtime-sciper.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
