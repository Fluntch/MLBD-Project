{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8af48ca1-b6d1-4092-b7b5-037d3c2d7aef",
   "metadata": {
    "id": "8af48ca1-b6d1-4092-b7b5-037d3c2d7aef"
   },
   "source": [
    "# M4 | Research Investigation Notebook\n",
    "\n",
    "In this notebook, you will do a research investigation of your chosen dataset in teams. You will begin by formally selecting your research question (task 0), then processing your data (task 1), creating a predictive model (task 2), evaluating your model's results (task 3), and describing the contributions of each team member (task 4).\n",
    "\n",
    "For grading, please make sure your notebook has all cells already run. You will also need to write a short, 2 page report about your design decisions as a team, to be uploaded to Moodle in the form of a PDF file next to this Jupyter notebook.\n",
    "\n",
    "You should provide arguments and justifications for all of your design decisions throughout this investigation. You can use your M3 responses as the basis for this discussion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82ea2d32-f0a9-4dc9-bb60-be43399f5b89",
   "metadata": {
    "id": "82ea2d32-f0a9-4dc9-bb60-be43399f5b89",
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Import the tables of the data set as dataframes.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from IPython.display import Image\n",
    "\n",
    "DATA_DIR = './data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89137355",
   "metadata": {},
   "source": [
    "## Task 0: Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dafc5b",
   "metadata": {},
   "source": [
    "*Our goal was to predict users performance based on their activity behaviour. To this end, we extracted different features as a proxy for their studying habits.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77f62b0-1945-48f1-8f22-5f6ebda1db8e",
   "metadata": {
    "id": "a77f62b0-1945-48f1-8f22-5f6ebda1db8e"
   },
   "source": [
    "## Task 1: Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85676347-da09-4247-a2af-0e033074006c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26480511-9115-48e7-bc9b-1b8a7a337cd6",
   "metadata": {},
   "source": [
    "Note: Since our preprocessing is quite extensive, we decided to *not* include it here in the notebook. Instead, we already provide you with the preprocessed data. \n",
    "This section will give you an overview of the employed preprocessing steps via statistics and visualisations. Feel free to inspect the preprocessing code by tracing it starting from `src/data_cleaning.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707d070d-c9d1-4f4e-a709-f886af8965ee",
   "metadata": {},
   "source": [
    "#### Removal of non-mappable courses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37345381-2fec-4da5-8e42-070f428c9d43",
   "metadata": {},
   "source": [
    "After discussing with Tom from Gogymi, we removed all data points with the course id 1696, 8117, 2019, 8117, 0, 2515, 2440. These courses were either used in the development of the platform or were the platform introduction course for students and teachers, respectively. This removed 7.1% of the data from `activity.csv`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408434d5-75a2-4ebf-8d61-3a02476b839a",
   "metadata": {},
   "source": [
    "#### Removal of non-first attempts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c26639-25b2-4708-aa8e-352efc66e164",
   "metadata": {},
   "source": [
    "We removed **1596 entries (24.81%)** from `all_scores` to retain only the **first exam attempt per user**.  \n",
    "This step is important because repeated attempts can distort the measurement of true performance:  \n",
    "students who retake an exam are likely to recall specific questions from earlier attempts, which gives them an advantage that doesn't reflect their initial understanding.  \n",
    "We therefore keep only the **chronologically first attempt** for each `(user_id, test_id)` pair."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76a4fa9-40eb-4c60-b15d-8c7af86403d2",
   "metadata": {},
   "source": [
    "#### Assigning domains to the activity and results data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad71906e-3479-4680-aa7f-06d758121992",
   "metadata": {},
   "source": [
    "The dataset included overview files for both activities and exam results. However, these files contained mixed data from the platform’s three domains: **math**, **text**, and **essay**.  \n",
    "We found it important to separate the data points by their respective domains to build a meaningful model.\n",
    "\n",
    "To achieve this, we created a mapping between test IDs and domains, based on the IDs found in `math_results.csv`, `essay_results.csv`, and `text_results.csv`. We then applied this mapping to both `activity.csv` and `all_scores.csv`.\n",
    "\n",
    "Initially, there were some overlaps between IDs assigned to math and essay domains. After discussing this issue with Tom, we received an updated version of `math_results.csv` (included in this project), which corrected these assignments. Specifically, the IDs **3301** and **5447** were originally assigned to both essay and math, but they should only belong to the essay domain.  \n",
    "**With this correction, we were able to successfully assign each ID to exactly one domain. As a result, all entries in both `all_scores.csv` and `activity.csv` could be mapped unambiguously.**\n",
    "\n",
    "The final domain distributions are:\n",
    "\n",
    "`activity.csv`\n",
    "- Math courses: **48.0%** of data\n",
    "- Text courses: **26.0%** of data\n",
    "- Essay courses: **26.0%** of data\n",
    "\n",
    "`all_scores.csv`\n",
    "- Math courses: **79.0%** of data\n",
    "- Text courses: **9.0%** of data\n",
    "- Essay courses: **12.0%** of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b39129-c136-4393-8a33-cba9a3a62eb5",
   "metadata": {},
   "source": [
    "#### Handling incorrect and missing dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ee7870-8a56-4987-83f0-7169f7e916df",
   "metadata": {},
   "source": [
    "`activity.csv`\n",
    "\n",
    "While inspecting the timestamps in `activity.csv`, we identified some incorrect entries.  \n",
    "A few activities had `activity_completed` dates from 1970, and in some cases, the recorded order of events was nonsensical — for example, activities appeared to finish before they started.\n",
    "\n",
    "To address these inconsistencies, we focused on rows where:\n",
    "- `activity_updated` < `activity_started`\n",
    "- or `activity_completed` < `activity_started`\n",
    "\n",
    "The findings:\n",
    "- **57** rows had `activity_updated` before `activity_started`\n",
    "- **1908** rows had `activity_completed` before `activity_started`\n",
    "- In total, about **2.82%** of the dataset contained invalid timestamps.\n",
    "\n",
    "We attempted to restore timestamps using `activity_updated` wherever possible.  \n",
    "- For **1851** rows, `activity_updated` was equal to or after `activity_started`, allowing us to infer a valid completion time.\n",
    "- However, for the remaining **57** rows, both `activity_updated` and `activity_completed` were before `activity_started`, making correction impossible.\n",
    "\n",
    "Given that these unresolved cases represent only **0.08%** of the data, we marked them as invalid and excluded them from any time-based analyses.\n",
    "\n",
    "`all_scores.csv`\n",
    "\n",
    "Since `all_scores.csv` only contains one time-related column (`date`) there could no be inconsistencies in date ordering. All dates lied within in a reasonable time range, which is why we did not perform any additional cleaning on them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53631a8c-3c9b-453e-8afa-926bd6f09e94",
   "metadata": {},
   "source": [
    "#### Time Coverage and Timestamp Consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6746361-b3ce-4df2-87c8-d77970927012",
   "metadata": {},
   "source": [
    "After cleaning inconsistent and unrealistic dates, we compared the timestamp ranges between `activity.csv` and `all_scores.csv`:\n",
    "\n",
    "|                | `activity_dates`         | `scores_dates`          |\n",
    "|----------------|---------------------------|--------------------------|\n",
    "| **Minimum**    | 2023-04-07 16:42:35        | 2023-04-07 17:26:54       |\n",
    "| **Maximum**    | 2025-03-07 14:55:42        | 2025-03-05 18:17:37       |\n",
    "| **Count**      | 173,097                    | 4,836                    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e46e0be-1774-4e3d-96c7-22edba2387d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='plots/data_cleaning/Weekly Distribution of Activity and Score Dates.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d499955f-fbc8-4abb-84be-8920023d8985",
   "metadata": {},
   "source": [
    "Key observations:\n",
    "- There are approximately **35.79 times** more timestamps in `activity.csv` than in `all_scores.csv`.\n",
    "- The dataset covers a total time span of **699 days**.\n",
    "- Both dataframes cover almost the exact same timeframe, with `activity.csv` covering two more dates\n",
    "- The timestamps in fall within a consistent and reasonable range (between 2023-04-07 and 2025-03-07).\n",
    "- The distribution of dates shows that most of the available data was recorded in the timeframe between July 2024 and March 2025\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dca0af-ef36-42d9-93fe-d57bce6b5859",
   "metadata": {},
   "source": [
    "#### Removal of activities with a duration of 0 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cefd0d-4f8a-4227-8fb4-f30521abf78d",
   "metadata": {
    "id": "34cefd0d-4f8a-4227-8fb4-f30521abf78d"
   },
   "outputs": [],
   "source": [
    "activity = pd.read_csv('{}/features/activity.csv'.format(DATA_DIR))\n",
    "performances = pd.read_csv('{}/features/performances.csv'.format(DATA_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72172beabeeaa0c7",
   "metadata": {},
   "source": [
    "Time spent on activities is one of the key behavioral features we use to analyze student behavior. While there are other features we consider, duration plays an important role in many of our methods.  \n",
    "Unfortunately, many activity entries have a recorded duration of zero, making them unusable for time-based analysis.  \n",
    "The following graph shows how many data points are affected by this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f014229d098b908b",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = activity.groupby('domain')['user_id'].count()\n",
    "unuseable = activity[activity['activity_completed'].isna()]\n",
    "unuseable_count = unuseable.groupby('domain')['user_id'].count()\n",
    "\n",
    "domains = sorted(set(total.index).union(set(unuseable_count.index)))\n",
    "total = total.reindex(domains, fill_value=0)\n",
    "unuseable_count = unuseable_count.reindex(domains, fill_value=0)\n",
    "\n",
    "x = np.arange(len(domains))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(x - width/2, total, width, label='Total')\n",
    "plt.bar(x + width/2, unuseable_count, width, label='Unusable')\n",
    "\n",
    "plt.ylabel('Number of Users')\n",
    "plt.xlabel('Domain')\n",
    "plt.title('Total vs Unusable Activity data by Domain')\n",
    "plt.xticks(x, domains, rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a77a7be-df9a-41ba-8af2-b012e6e3657f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unuseable_count.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e03d0416ee90bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'By deleting these colums, we lose {(unuseable_count.sum()/total.sum() * 100).round(2)} % of the initial data ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3452b083fb3f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "activity.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abe62a809be614d",
   "metadata": {},
   "source": [
    "**Comment:** This is quite a big loss of data but a necessary one... (see annexe for trial without this loss of data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37356e2-fc27-459c-b8f0-9967d28fc02b",
   "metadata": {},
   "source": [
    "#### Summary: Total Data Retention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdeda0de-7d5f-4295-bd97-0135afb805a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"plots/data_cleaning/Data Retention Over Cleaning Steps.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594a4cb7-a83d-4d3f-b87e-b9b986ad375d",
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_retain = round(29892/72897, 2) * 100 \n",
    "all_scores_retain = round(4836/6432, 2) * 100\n",
    "print(f\"In total, we retained {activity_retain}% of the activity data, and {all_scores_retain}% of the all_scores data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bfd03d50b4318a",
   "metadata": {},
   "source": [
    "### Feature engineering \n",
    "\n",
    "**Our aim is to predict a user's score on their first attempt at a specific exam, based on their behavior leading up to it. To train our model, we engineered the following features to capture and characterize user behavior:**\n",
    "\n",
    "- recent_avg_time_per_activity\n",
    "\n",
    "*The average time (in minutes) spent per activity during the last 10 days before the exam, including the exam day. This reflects how intensely the user has been engaging recently.*\n",
    "\n",
    "- days_since_last_activity\n",
    "\n",
    "*The number of days between the user's most recent activity and the exam. A higher value may indicate a longer gap in studying before the exam.*\n",
    "\n",
    "- total_time_spent_on_activity_before_exam\n",
    "\n",
    "*The total amount of time (in minutes) the user spent on activities prior to the exam. This gives a sense of overall study investment leading up to the test.*\n",
    "\n",
    "- average_performance_past_exams\n",
    "\n",
    "*The mean performance from the user's previous exams (before this one). This can provide a rough estimate of the user's historical performance.*\n",
    "\n",
    "- avg_activities_per_day_recent\n",
    "\n",
    "*The average number of activities completed per day in the 10-day rolling window. A higher number may suggest more consistent or intense preparation.*\n",
    "\n",
    "- active_days_ratio_recent\n",
    "\n",
    "*The proportion of days (out of 10) on which the user was active. This indicates how regularly they studied leading up to the exam.*\n",
    "\n",
    "- diversity_recent\n",
    "\n",
    "*The number of unique activity types completed in the rolling window. Higher diversity could reflect more varied engagement with different learning methods or content types.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175a3427bf7828d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling window for recent activity\n",
    "rolling_window_days = 10\n",
    "\n",
    "\n",
    "def compute_all_features_for_exam(exam_row, user_activities, user_exams, window_days=rolling_window_days):\n",
    "\n",
    "    exam_dt = exam_row['time']\n",
    "\n",
    "\n",
    "    # Include activities up to and including exam_date\n",
    "    previous_activities = user_activities[user_activities['activity_updated'] < exam_dt].copy()\n",
    "\n",
    "\n",
    "    # Rolling window (activities in the last N days, including exam day)\n",
    "    window_start = exam_dt - pd.Timedelta(days=window_days)\n",
    "    rolling_activities = previous_activities[previous_activities['activity_updated'] >= window_start].copy()\n",
    "\n",
    "    features = {}\n",
    "\n",
    "\n",
    "    # Recent average time per activity (rolling window)\n",
    "    total_time_rolling = rolling_activities['time_in_minutes'].sum()\n",
    "    count_rolling = len(rolling_activities)\n",
    "    features['recent_avg_time_per_activity'] = total_time_rolling / count_rolling if count_rolling > 0 else 0\n",
    "\n",
    "\n",
    "    # Number of days since last activity\n",
    "    if not previous_activities.empty:\n",
    "        last_activity_date = previous_activities['activity_updated'].max()\n",
    "        features['days_since_last_activity'] = (exam_dt - last_activity_date).days\n",
    "    else:\n",
    "        features['days_since_last_activity'] = np.nan\n",
    "\n",
    "\n",
    "    # Total time spent on activities before the exam\n",
    "    features['total_time_spent_on_activity_before_exam'] = previous_activities['time_in_minutes'].sum() if not previous_activities.empty else 0\n",
    "\n",
    "\n",
    "    # Average performance on past exams\n",
    "    previous_exams = user_exams[user_exams['time'] < exam_dt]\n",
    "    features['average_performance_past_exams'] = previous_exams['performance'].mean() if not previous_exams.empty else np.nan\n",
    "\n",
    "\n",
    "    # Usage Frequency: Average activities per day in rolling window & Active days ratio\n",
    "    features['avg_activities_per_day_recent'] = count_rolling / window_days if window_days > 0 else np.nan\n",
    "    if not rolling_activities.empty:\n",
    "        distinct_days = rolling_activities['activity_updated'].dt.normalize().nunique()\n",
    "    else:\n",
    "        distinct_days = 0\n",
    "    features['active_days_ratio_recent'] = distinct_days / window_days if window_days > 0 else np.nan\n",
    "\n",
    "\n",
    "    # Activity diversity (rolling window)\n",
    "    features['diversity_recent'] = rolling_activities['activity_type'].nunique() if not rolling_activities.empty else 0\n",
    "\n",
    "\n",
    "    return pd.Series(features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9130556f6c1e9f3d",
   "metadata": {},
   "source": [
    "**All features are calculated separately within each domain — Math, Text, and Essay — to capture domain-specific learning patterns and engagement.**\n",
    "\n",
    "We have also scaled the features using a standard because we assume a normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4adfa9ea381bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "columns_to_scale = ['recent_avg_time_per_activity', 'days_since_last_activity', 'total_time_spent_on_activity_before_exam','average_performance_past_exams','avg_activities_per_day_recent','diversity_recent']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f96e69f99cc5e0",
   "metadata": {},
   "source": [
    "#### Math data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf0ba7985abb23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "performances_math = performances[performances['domain']== 'math'].copy()\n",
    "activity_math = activity[activity['domain']== 'math'].copy()\n",
    "\n",
    "# Convert the 'date' columns to datetime\n",
    "activity_math['activity_updated'] = pd.to_datetime(activity_math['activity_updated'])\n",
    "performances_math['time'] = pd.to_datetime(performances_math['time'])\n",
    "\n",
    "# Loop over each exam (grouped by user) in performances_math and compute all features.\n",
    "features_list = []\n",
    "\n",
    "for user_id, user_exams in performances_math.groupby('user_id'):\n",
    "    # Get corresponding activities for the user from activity_math and sort by date\n",
    "    user_activities = activity_math[activity_math['user_id'] == user_id].sort_values('activity_updated')\n",
    "    user_exams_sorted = user_exams.sort_values('time')\n",
    "\n",
    "    for exam_index, exam_row in user_exams_sorted.iterrows():\n",
    "        feats = compute_all_features_for_exam(exam_row, user_activities, user_exams_sorted, rolling_window_days)\n",
    "        feats['exam_index'] = exam_index\n",
    "        features_list.append(feats)\n",
    "\n",
    "# Output df\n",
    "features_df = pd.DataFrame(features_list).set_index('exam_index')\n",
    "performances_math_features = performances_math.join(features_df, how='left')\n",
    "\n",
    "\n",
    "# scaling the columns\n",
    "scaled_values = scaler.fit_transform(performances_math_features[columns_to_scale])\n",
    "scaled_df = pd.DataFrame(scaled_values, columns=columns_to_scale, index=performances_math_features.index)\n",
    "remaining_df = performances_math_features.drop(columns=columns_to_scale)\n",
    "final_df_math = pd.concat([scaled_df, remaining_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c272392980aad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_math.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b281cff5428e4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Nb of rows :', final_df_math.user_id.count())\n",
    "print('Nb of rows where we are missing feature values :', final_df_math.isna().any(axis=1).sum())\n",
    "print('Percentage of rows where we are missing feature values :', final_df_math.isna().any(axis=1).sum()/ final_df_math.user_id.count() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d067bf9f5f4e0bd",
   "metadata": {},
   "source": [
    "**Comment:** Some features for certain rows cannot be computed, particularly the \"average_performance_past_exams\" feature. This occurs for the first exam questions attempted by each user, where no prior performance data is available.\n",
    "\n",
    "**Comment:** For the math dataset, approximately 12% of the data is affected by this issue. However, if we were to remove this data the loss is minor and there are still around 3400 healthy data points that remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9daf4d43cdcb5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\n",
    "    'recent_avg_time_per_activity',\n",
    "    'days_since_last_activity',\n",
    "    'total_time_spent_on_activity_before_exam',\n",
    "    'average_performance_past_exams',\n",
    "    'avg_activities_per_day_recent',\n",
    "    'diversity_recent',\n",
    "    'active_days_ratio_recent',\n",
    "    'performance'  \n",
    "]\n",
    "\n",
    "correlation_matrix_math = final_df_math[feature_columns].corr()\n",
    "\n",
    "plt.figure(figsize=(8, 7))\n",
    "sns.heatmap(correlation_matrix_math, annot=True, fmt=\".2f\", cmap='coolwarm', center=0)\n",
    "plt.title(\"Correlation Matrix of Features (Math Domain)\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04c7f49834fe881",
   "metadata": {},
   "source": [
    "#### Essay data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1b44b8a2762a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "performances_essay = performances[performances['domain']== 'essay'].copy()\n",
    "activity_essay= activity[activity['domain']== 'essay'].copy()\n",
    "\n",
    "# Convert the date columns  to datetime\n",
    "activity_essay['activity_updated'] = pd.to_datetime(activity_essay['activity_updated'])\n",
    "performances_essay['time'] = pd.to_datetime(performances_essay['time'])\n",
    "\n",
    "# Loop over each exam (grouped by user) in performances_essay and compute all features.\n",
    "features_list = []\n",
    "\n",
    "for user_id, user_exams in performances_essay.groupby('user_id'):\n",
    "    # Get corresponding activities for the user from activity_essay and sort by date\n",
    "    user_activities = activity_essay[activity_essay['user_id'] == user_id].sort_values('date')\n",
    "    user_exams_sorted = user_exams.sort_values('date')\n",
    "\n",
    "    for exam_index, exam_row in user_exams_sorted.iterrows():\n",
    "        feats = compute_all_features_for_exam(exam_row, user_activities, user_exams_sorted, rolling_window_days)\n",
    "        feats['exam_index'] = exam_index\n",
    "        features_list.append(feats)\n",
    "\n",
    "# Output df\n",
    "features_df = pd.DataFrame(features_list).set_index('exam_index')\n",
    "performances_essay_features = performances_essay.join(features_df, how='left')\n",
    "\n",
    "\n",
    "# scaling the columns\n",
    "scaled_values = scaler.fit_transform(performances_essay_features[columns_to_scale])\n",
    "scaled_df = pd.DataFrame(scaled_values, columns=columns_to_scale, index=performances_essay_features.index)\n",
    "remaining_df = performances_essay_features.drop(columns=columns_to_scale)\n",
    "final_df_essay = pd.concat([scaled_df, remaining_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d4a5a1d21f6af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_essay.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4dfa2454cc5537",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Nb of rows :', final_df_essay.user_id.count())\n",
    "print('Nb of rows where we are missing feature values :', final_df_essay.isna().any(axis=1).sum())\n",
    "print('Percentage of rows where we are missing feature values :', final_df_essay.isna().any(axis=1).sum()/ final_df_essay.user_id.count() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3d566af4e9d869",
   "metadata": {},
   "source": [
    "**Comment:** The essay domain has very limited data available, with only 581 rows usable for training. \n",
    "\n",
    "**Comment:** Many rows are also missing the \"average_performance_past_exams\" feature. These are missing for the same reason as previously mentionned. Due to the already limited amount of data, this will prove to be problematic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3c0360f5417381",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix_essay = final_df_essay[feature_columns].corr()\n",
    "\n",
    "plt.figure(figsize=(8, 7))\n",
    "sns.heatmap(correlation_matrix_essay, annot=True, fmt=\".2f\", cmap='coolwarm', center=0)\n",
    "plt.title(\"Correlation Matrix of Features (Essay Domain)\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bec388555e7b4",
   "metadata": {},
   "source": [
    "#### Text data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649551d4575a9bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "performances_text = performances[performances['domain']== 'text'].copy()\n",
    "activity_text= activity[activity['domain']== 'text'].copy()\n",
    "\n",
    "# Convert the date columns to datetime\n",
    "activity_text['activity_updated'] = pd.to_datetime(activity_text['activity_updated'])\n",
    "performances_text['time'] = pd.to_datetime(performances_text['time'])\n",
    "\n",
    "# Loop over each exam (grouped by user) in performances_text and compute all features.\n",
    "features_list = []\n",
    "\n",
    "for user_id, user_exams in performances_text.groupby('user_id'):\n",
    "    # Get corresponding activities for the user from activity_text and sort by date\n",
    "    user_activities = activity_text[activity_text['user_id'] == user_id].sort_values('date')\n",
    "    user_exams_sorted = user_exams.sort_values('date')\n",
    "\n",
    "    for exam_index, exam_row in user_exams_sorted.iterrows():\n",
    "        feats = compute_all_features_for_exam(exam_row, user_activities, user_exams_sorted, rolling_window_days)\n",
    "        feats['exam_index'] = exam_index\n",
    "        features_list.append(feats)\n",
    "\n",
    "# Output df\n",
    "features_df = pd.DataFrame(features_list).set_index('exam_index')\n",
    "performances_text_features = performances_text.join(features_df, how='left')\n",
    "\n",
    "\n",
    "# Scale the columns\n",
    "scaled_values = scaler.fit_transform(performances_text_features[columns_to_scale])\n",
    "scaled_df = pd.DataFrame(scaled_values, columns=columns_to_scale, index=performances_text_features.index)\n",
    "remaining_df = performances_text_features.drop(columns=columns_to_scale)\n",
    "final_df_text = pd.concat([scaled_df, remaining_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf98fa308b819e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_text.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b35842bf17b128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Nb of rows :', final_df_text.user_id.count())\n",
    "print('Nb of rows where we are missing feature values :', final_df_text.isna().any(axis=1).sum())\n",
    "print('Percentage of rows where we are missing feature values :', final_df_text.isna().any(axis=1).sum()/ final_df_text.user_id.count() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1470465c29e47014",
   "metadata": {},
   "source": [
    "**Comment:** The text domain has even less data available, with only 445 rows usable for training. \n",
    "\n",
    "**Comment:** Approximately 80% of the rows are missing with most coming from \"days_since_last_activity\". This means that users took the exam with no prior activity logs, meaning that, for our research question, these rows are useless since we are trying to predict performance based on past activity. There are also many rows where the \"average_performance_past_exams\" feature is missing, which will prove problematic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981f0deaba2c5cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix_text = final_df_essay[feature_columns].corr()\n",
    "\n",
    "plt.figure(figsize=(8, 7))\n",
    "sns.heatmap(correlation_matrix_text, annot=True, fmt=\".2f\", cmap='coolwarm', center=0)\n",
    "plt.title(\"Correlation Matrix of Features (Text Domain)\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d0fcca-686e-47a3-8567-c3f92db187d4",
   "metadata": {
    "id": "76d0fcca-686e-47a3-8567-c3f92db187d4"
   },
   "source": [
    "*Your discussion about your processing decisions goes here*\n",
    "\n",
    "\n",
    "When looking at the correlation analysis, we observe that the feature most strongly correlated with performance is \"average_performance_past_exams.\" This presents a significant problem for the essay and text datasets, where not only are there very few rows to begin with, but this important feature is often missing. As a result, we decided to focus our efforts on the math data, where we have a much healthier number of data points available.\n",
    "\n",
    "We did attempt to train models on the essay and text data; however, the performance was very poor, likely due to the limited amount of training examples and the missing key features. Training a model with so little data is highly unreliable and unlikely to generalize well. On the other hand, if our approach works well on the math data, it suggests that the methodology could be transferable to other domains, provided enough data points are available to fit a model properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe9e40aeb240184",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7574678e04516dc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7914f719a93faf9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4d2d5ac04c6666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85633adb-d317-4ee3-bf06-e9f82f589c41",
   "metadata": {
    "id": "85633adb-d317-4ee3-bf06-e9f82f589c41"
   },
   "source": [
    "## Task 2: Model Building\n",
    "\n",
    "Train a model for your research question. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce83f76b3bc64b4",
   "metadata": {},
   "source": [
    "**Comment:** Seeing the lack of data available to us for both the text and essay domains, we decided to concentrate on the math data for the following parts. Therefore all models are solely for the math data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603838ee342930f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2c58650d5ecfc72",
   "metadata": {},
   "source": [
    "### Method 0 : Simple Linear Regression where we drop NAN\n",
    "\n",
    "Sometimes, the simplest approaches can be the most effective. For our baseline model, we began by dropping any rows containing missing values (NaNs) to ensure a clean dataset. We then applied a simple linear regression model to predict exam performance based on the available features. This served as a straightforward benchmark to compare against more complex models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b65ebd-c148-4ae8-833e-018411eeda86",
   "metadata": {
    "id": "90b65ebd-c148-4ae8-833e-018411eeda86"
   },
   "outputs": [],
   "source": [
    "final_df_math_drop = final_df_math.copy()\n",
    "final_df_math_drop.dropna(inplace=True)\n",
    "\n",
    "\n",
    "# Linear Regression Model\n",
    "mod_method0 = smf.ols(formula='performance ~  recent_avg_time_per_activity + days_since_last_activity + total_time_spent_on_activity_before_exam + average_performance_past_exams + avg_activities_per_day_recent + diversity_recent', data=final_df_math_drop)\n",
    "# Fit the model\n",
    "res_method0 = mod_method0.fit()\n",
    "\n",
    "# Print regression results summary \n",
    "#print(res_method0.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1bd0c800c4a9e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff9ea02c8a842895",
   "metadata": {},
   "source": [
    "### Method 1 : Mixed Effects Linear Regression where we drop NAN and cluster the users \n",
    "\n",
    "To enhance our simple linear regression model, we employed a K-Means clustering approach to group users based on their feature profiles. This allowed us to account for heterogeneity among users by capturing patterns in behavior and performance. By incorporating these clusters as mixed effects, we aimed to improve the model’s ability to generalize across diverse user groups and better explain variations in exam outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c148ded5e6536b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collapse to one row per user for clustering\n",
    "user_feats = final_df_math_drop.groupby('user_id')[['recent_avg_time_per_activity', 'days_since_last_activity', 'total_time_spent_on_activity_before_exam', 'average_performance_past_exams', 'avg_activities_per_day_recent', 'active_days_ratio_recent', 'diversity_recent']].mean().reset_index()\n",
    "\n",
    "# Cluster into 3 groups\n",
    "kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "user_feats['cluster'] = kmeans.fit_predict(user_feats.drop(columns='user_id'))\n",
    "\n",
    "# Merge cluster label back into final_df\n",
    "df2 = final_df_math_drop.merge(user_feats[['user_id','cluster']], on='user_id')\n",
    "\n",
    "# Fit a mixed‐effects model with random intercept per cluster\n",
    "mod_method1 = smf.mixedlm(\"performance ~ recent_avg_time_per_activity + days_since_last_activity + \\\n",
    "     total_time_spent_on_activity_before_exam + average_performance_past_exams + \\\n",
    "     avg_activities_per_day_recent + active_days_ratio_recent + diversity_recent\",df2,groups=\"cluster\")\n",
    "res_method1 = mod_method1.fit(reml=False)\n",
    "#print(res_method1.summary())\n",
    "\n",
    "# In-sample predictions\n",
    "df2[\"pred_cluster\"] = res_method1.predict(df2)\n",
    "\n",
    "# Compute R² and RMSE\n",
    "# r2_cluster = r2_score(df2[\"performance\"], df2[\"pred_cluster\"])\n",
    "# rmse_cluster = mean_squared_error(df2[\"performance\"], df2[\"pred_cluster\"])\n",
    "\n",
    "# print(\"Cluster‐model  AIC:\",  mdf.aic, \"  BIC:\", mdf.bic)\n",
    "# print(f\"Cluster‐model  R² = {r2_cluster:.3f},  RMSE = {rmse_cluster:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10a0b0583c3aaaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d2bc959671a6296",
   "metadata": {},
   "source": [
    "### Method 2 : Simple Linear Regression where we impute the missing NAN values with the median value for that exam\n",
    "\n",
    "\n",
    "For this method we used a simple linear regression model with median imputation for missing values. Specifically, any NaN values were filled with the median value for the corresponding feature within each exam. This approach preserved more data compared to dropping rows and allowed the model to leverage a broader training set while maintaining simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0cafdc887f1472",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_math_median = performances_math_features.copy()\n",
    "\n",
    "# Fill by test_id median\n",
    "performance_math_median['average_performance_past_exams'] = (\n",
    "    performance_math_median\n",
    "    .groupby('test_id')['average_performance_past_exams']\n",
    "    .transform(lambda x: x.fillna(x.median()))\n",
    "    .fillna(performance_math_median['average_performance_past_exams'].median())   # fallback to global median if a test has no median\n",
    ")\n",
    "\n",
    "# scaling the columns\n",
    "columns_to_scale = ['recent_avg_time_per_activity', 'days_since_last_activity', 'total_time_spent_on_activity_before_exam','average_performance_past_exams','avg_activities_per_day_recent','active_days_ratio_recent','diversity_recent']\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_values = scaler.fit_transform(performance_math_median[columns_to_scale])\n",
    "scaled_df = pd.DataFrame(scaled_values, columns=columns_to_scale, index=performance_math_median.index)\n",
    "remaining_df = performance_math_median.drop(columns=columns_to_scale)\n",
    "final_df_med = pd.concat([scaled_df, remaining_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76deeef53acb4c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Model\n",
    "mod_method2 = smf.ols(formula= 'performance ~  recent_avg_time_per_activity + days_since_last_activity + total_time_spent_on_activity_before_exam + average_performance_past_exams + avg_activities_per_day_recent + active_days_ratio_recent + diversity_recent', data=final_df_med)\n",
    "\n",
    "# Fit the model\n",
    "res_method2 = mod_method2.fit()\n",
    "\n",
    "# Print regression results summary\n",
    "#print(res_method2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443c10abffde9d83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d67831a40e0fb2d",
   "metadata": {},
   "source": [
    "### Method 3 : Simple Linear Regression where we impute the missing NAN values using KNN\n",
    "\n",
    "For Method 3, we applied a simple linear regression model with K-Nearest Neighbors (KNN) imputation for missing values. Instead of using global statistics like the median, KNN imputation estimates missing values based on the most similar observations in the dataset. This method allows for more context-aware imputations, potentially preserving underlying data relationships and improving the model’s predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e189a4725249d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_math_impute = performances_math_features.copy()\n",
    "\n",
    "# Features we’ll use to compute similarity:\n",
    "sim_features = ['recent_avg_time_per_activity','days_since_last_activity','total_time_spent_on_activity_before_exam',\n",
    "                'avg_activities_per_day_recent','active_days_ratio_recent','diversity_recent']\n",
    "\n",
    "# New column to hold the imputed values\n",
    "performance_math_impute['avg_perf_past_exams_imputed'] = performance_math_impute['average_performance_past_exams']\n",
    "\n",
    "# Group by test_id and run kNN inside each group\n",
    "for test_id, group in performance_math_impute.groupby('test_id'):\n",
    "    # indices of rows we need to fill\n",
    "    missing_idx = group[group['average_performance_past_exams'].isna()].index\n",
    "    if len(missing_idx) == 0:\n",
    "        continue\n",
    "\n",
    "    # candidate neighbors: same test, non‐missing avg_pct\n",
    "    candidates = group[group['average_performance_past_exams'].notna()]\n",
    "    if candidates.shape[0] == 0:\n",
    "        # if no one else took that test --> impute median\n",
    "        continue\n",
    "\n",
    "    # Matrix of sim_features, impute median for remaining NaNs\n",
    "    feat_mat = group[sim_features].copy()\n",
    "    feat_mat = feat_mat.fillna(feat_mat.median())\n",
    "\n",
    "    # Split into X_train (candidates) and X_query (the missing rows)\n",
    "    X_train = feat_mat.loc[candidates.index].values\n",
    "    X_query = feat_mat.loc[missing_idx].values\n",
    "\n",
    "    # Use up to 3 neighbors (fewer if not enough candidates)\n",
    "    k = min(3, X_train.shape[0])\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, algorithm='auto').fit(X_train)\n",
    "    distances, neighbors = nbrs.kneighbors(X_query)\n",
    "\n",
    "    # For each missing row, average the actual test scores of its neighbors\n",
    "    candidate_scores = candidates['performance'].values\n",
    "    for i, idx in enumerate(missing_idx):\n",
    "        nbr_idxs = neighbors[i]\n",
    "        imputed_val = candidate_scores[nbr_idxs].mean()\n",
    "        performance_math_impute.at[idx, 'avg_perf_past_exams_imputed'] = imputed_val\n",
    "\n",
    "# Replace the original column\n",
    "performance_math_impute['average_performance_past_exams'] = performance_math_impute['avg_perf_past_exams_imputed']\n",
    "performance_math_impute.drop(columns='avg_perf_past_exams_imputed', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b74fc77c55f898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the columns\n",
    "columns_to_scale = ['recent_avg_time_per_activity', 'days_since_last_activity', 'total_time_spent_on_activity_before_exam','average_performance_past_exams','avg_activities_per_day_recent','active_days_ratio_recent','diversity_recent']\n",
    "scaler = StandardScaler()\n",
    "scaled_values = scaler.fit_transform(performance_math_impute[columns_to_scale])\n",
    "scaled_df = pd.DataFrame(scaled_values, columns=columns_to_scale, index=performance_math_impute.index)\n",
    "remaining_df = performance_math_impute.drop(columns=columns_to_scale)\n",
    "final_df_impute = pd.concat([scaled_df, remaining_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7253f001e8b17dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_impute.dropna(inplace=True) # drop the remaining NaNs\n",
    "\n",
    "# Linear Regression Model\n",
    "mod_method3 = smf.ols(formula= 'performance ~  recent_avg_time_per_activity + days_since_last_activity + total_time_spent_on_activity_before_exam + average_performance_past_exams + avg_activities_per_day_recent + active_days_ratio_recent + diversity_recent', data=final_df_impute)\n",
    "\n",
    "# Fit the model\n",
    "res_method3 = mod_method3.fit()\n",
    "\n",
    "# Print regression results summary\n",
    "#print(res_method3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d52ee16998c9242",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b2ad4d16cf86d5d",
   "metadata": {},
   "source": [
    "### Method 4 : Gradient Boosting Regressor that accepts NAN values \n",
    "\n",
    "For Method 4, we used a Gradient Boosting Regressor that natively handles missing values. Unlike simpler models, gradient boosting can internally learn how to deal with NaNs during training, making it well-suited for real-world, imperfect data. This approach allowed us to retain the full dataset without requiring explicit imputation, while benefiting from the model's ability to capture complex, non-linear relationships in the features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38708f56015fff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = performances_math_features.copy()\n",
    "\n",
    "feature_cols = ['recent_avg_time_per_activity','days_since_last_activity','total_time_spent_on_activity_before_exam','average_performance_past_exams',\n",
    "    'avg_activities_per_day_recent','active_days_ratio_recent','diversity_recent']\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df['performance']\n",
    "\n",
    "# Train-Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Pipeline: impute → scale → gradient boost\n",
    "pipeline = Pipeline([\n",
    "    ('imputer',   SimpleImputer(strategy='median')),\n",
    "    ('scaler',    StandardScaler()),\n",
    "    ('gbr',       GradientBoostingRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Grid search for key hyperparameters\n",
    "param_grid = {'gbr__n_estimators': [100, 200], 'gbr__learning_rate': [0.05, 0.1], 'gbr__max_depth':[3, 5], 'gbr__subsample':[0.8, 1.0]}\n",
    "grid = GridSearchCV(pipeline, param_grid, cv=5, scoring='r2', n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit the model\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "# print(\"Test R²:\", r2_score(y_test, y_pred))\n",
    "# print(\"Test RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "# Cross‑validated performance\n",
    "# cv_scores = cross_val_score(best_model, X, y, cv=5, scoring='r2', n_jobs=-1)\n",
    "# print(\"5‑fold CV R²: %0.3f ± %0.3f\" % (cv_scores.mean(), cv_scores.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda1ec4d33f9b6f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f20d763b9aa964",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d99da61aa2d4cba",
   "metadata": {},
   "source": [
    "*Your discussion about your model training goes here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a59aefef1231f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999ba20554e71d4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7ff09aa3bbce46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3c9a655-cec9-4c57-aec7-7a982f57a3af",
   "metadata": {
    "id": "b3c9a655-cec9-4c57-aec7-7a982f57a3af"
   },
   "source": [
    "## Task 3: Model Evaluation\n",
    "In this task, you will use metrics to evaluate your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e52f2227d0ccc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db55845a4c11d98b",
   "metadata": {},
   "source": [
    "### Model 0 : \n",
    "\n",
    "Simple Linear Regression where we drop NAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdae6ff5d8c8dc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print regression results summary\n",
    "print(res_method0.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9dd8e4649d10",
   "metadata": {},
   "source": [
    "**Comment:** The simple linear regression model where we drop the NAN's has an R-squared of 0.267, meaning it explains 26.7% of the variance in performance. While this is a relatively modest value, it's actually quite good for real world data, which tends to be noisy and influenced by factors not captured in the model. The significant predictors, such as \"recent_average_time_per_activity\" and \"total_time_spent_on_activity_before_exam\" suggest the model is capturing meaningful patterns. The \"average_performance_in_past_exams\" feature is also significant and has the largest coefficient. This reinforces our decision to focus on the math data, as this feature is less available in the essay and text domain data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb92808d487dba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Not convinced about this plot -----> keep it or not ?\n",
    "\n",
    "\n",
    "final_df_math_drop['predicted_performance'] = res_method0.fittedvalues\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "sns.scatterplot(x='average_performance_past_exams', y='performance', data=final_df_math_drop, color='orange', label='Actual Performance')\n",
    "sns.regplot(x='average_performance_past_exams', y='predicted_performance', data=final_df_math_drop, scatter_kws={'s': 10}, line_kws={'color': 'red'}, label='Predicted Performance')\n",
    "\n",
    "plt.xlabel('Average Performance in Past Exams')\n",
    "plt.ylabel('Predicted Performance')\n",
    "plt.title('Predicted vs. Actual Student Performance Based on Past Exam Averages')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8927d148b552596d",
   "metadata": {},
   "source": [
    "**Comment:** The plot illustrates the relationship between the \"average_performance_in_past_exams\" and the model’s predicted performance. This feature showed the strongest correlation with actual performance in the preliminary analysis and also emerged as the most significant predictor in our model, with the largest coefficient. Visualizing its impact helps us understand how well the model captures this relationship. As seen in the plot, there is a clear upward trend in the actual performance data, and the model has effectively captured this pattern in its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74c6af98d0be397",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "sns.scatterplot(x='predicted_performance', y='performance', data=final_df_math_drop, color='dodgerblue')\n",
    "\n",
    "min_val = min(final_df_math_drop['performance'].min(), final_df_math_drop['predicted_performance'].min())\n",
    "max_val = max(final_df_math_drop['performance'].max(), final_df_math_drop['predicted_performance'].max())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], ls=\"--\", color=\"gray\", label='Perfect Prediction')\n",
    "\n",
    "buffer = 20\n",
    "plt.plot([min_val, max_val], [min_val + buffer, max_val + buffer], ls=\"--\", color=\"red\", alpha=0.5, label=f'+{buffer} Margin')\n",
    "plt.plot([min_val, max_val], [min_val - buffer, max_val - buffer], ls=\"--\", color=\"red\", alpha=0.5, label=f'-{buffer} Margin')\n",
    "\n",
    "plt.xlabel('Actual Performance')\n",
    "plt.ylabel('Predicted Performance')\n",
    "plt.title(\"Predicted vs Actual Performance per Cluster\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a6541304084e3a",
   "metadata": {},
   "source": [
    "**Comment:** By examining the model’s predictions against the actual performance values, we gain a better understanding of the R-squared value. The model does capture the overall trend in performance reasonably well, but there is still a noticeable amount of error. This level of imperfection is expected, given the complexity and variability in the data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752a40791c868f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate means\n",
    "mean_actual = final_df_math_drop[\"performance\"].mean()\n",
    "mean_pred = final_df_math_drop[\"predicted_performance\"].mean()\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "# Scatter plot\n",
    "sns.scatterplot(x='predicted_performance', y='performance', data=final_df_math_drop, color='dodgerblue', alpha=0.7)\n",
    "\n",
    "# Add mean lines\n",
    "plt.axhline(mean_actual, color=\"gray\", linestyle=\"--\", linewidth=1)\n",
    "plt.axvline(mean_pred, color=\"gray\", linestyle=\"--\", linewidth=1)\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel('Predicted Performance')\n",
    "plt.ylabel('Actual Performance')\n",
    "plt.title('Quadrant Analysis: Direction of Prediction vs Actual')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb4f6d151cecc8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91026014a5339dfa",
   "metadata": {},
   "source": [
    "### Model 1 :\n",
    "\n",
    "Mixed Effects Linear Regression where we drop NAN and cluster the users "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb177b44ec578c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res_method1.summary())\n",
    "\n",
    "# Compute R² and RMSE\n",
    "r2_cluster = r2_score(df2[\"performance\"], df2[\"pred_cluster\"])\n",
    "rmse_cluster = mean_squared_error(df2[\"performance\"], df2[\"pred_cluster\"])\n",
    "\n",
    "print(\"Cluster‐model  AIC:\",  res_method1.aic, \"  BIC:\", res_method1.bic)\n",
    "print(f\"Cluster‐model  R² = {r2_cluster:.3f},  RMSE = {rmse_cluster:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf38afa0c99e51cf",
   "metadata": {},
   "source": [
    "**Comment:** The mixed effects linear regression model has a R-squared value of 0.252. This is surprisingly lower than the simpler model 0, but not by much and it still seems to perform fairly well. For the significant features, \"average_performance_past_exams\" again seems to be the most prevelant feature, with other like \"avg_activities_per_day_recent\" and \"recent_avg_time_per_activity\" also contributing postiviely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1365a4f6746598c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = 20\n",
    "\n",
    "g = sns.FacetGrid(df2, col=\"cluster\", col_wrap=3, height=5, sharex=True, sharey=True)\n",
    "\n",
    "g.map_dataframe(sns.scatterplot, x=\"performance\", y=\"pred_cluster\", color='dodgerblue')\n",
    "\n",
    "min_val = min(df2[\"performance\"].min(), df2[\"pred_cluster\"].min())\n",
    "max_val = max(df2[\"performance\"].max(), df2[\"pred_cluster\"].max())\n",
    "\n",
    "for i, ax in enumerate(g.axes.flatten()):\n",
    "    \n",
    "    ax.plot([min_val, max_val], [min_val, max_val], ls=\"--\", color=\"gray\",\n",
    "            label=\"Perfect Prediction\" if i == 0 else \"\")\n",
    "    \n",
    "    ax.plot([min_val, max_val], [min_val + buffer, max_val + buffer], ls=\"--\", color=\"red\", alpha=0.5,\n",
    "            label=f'+{buffer} Margin' if i == 0 else \"\")\n",
    "    ax.plot([min_val, max_val], [min_val - buffer, max_val - buffer], ls=\"--\", color=\"red\", alpha=0.5,\n",
    "            label=f'-{buffer} Margin' if i == 0 else \"\")\n",
    "\n",
    "g.axes[0].legend()\n",
    "\n",
    "g.set_axis_labels(\"Actual Performance\", \"Predicted Performance\")\n",
    "g.set_titles(\"Cluster {col_name}\")\n",
    "g.fig.suptitle(\"Predicted vs Actual Performance per Cluster\", fontsize=16, y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae786808bc68189c",
   "metadata": {},
   "source": [
    "**Comment:** ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a88b0138235e1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "964892e813d777d6",
   "metadata": {},
   "source": [
    "### Model 2 :\n",
    "\n",
    "Simple Linear Regression where we impute the missing NAN values with the median value for that exam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbdd64593e3fd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res_method2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea1d11c4310cd47",
   "metadata": {},
   "source": [
    "**Comment:** The simple linear regression model with imputed NAN's has a R-squared value of 0.242. This is notably lower than both of the previous models. Among the significant predictors, \"average_performance_past_exams\" remains the most influential, followed by \"recent_avg_time_per_activity\" and \"total_time_spent_on_activity_before_exam\", all of which contribute positively to the model's predictive capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9623e50abdf7527",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_med['predicted_performance'] = res_method2.fittedvalues\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "sns.scatterplot(x='predicted_performance', y='performance', data=final_df_med, color='dodgerblue')\n",
    "\n",
    "min_val = min(final_df_math_drop['performance'].min(), final_df_math_drop['predicted_performance'].min())\n",
    "max_val = max(final_df_math_drop['performance'].max(), final_df_math_drop['predicted_performance'].max())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], ls=\"--\", color=\"gray\")\n",
    "\n",
    "buffer = 20\n",
    "plt.plot([min_val, max_val], [min_val + buffer, max_val + buffer], ls=\"--\", color=\"red\", alpha=0.5, label=f'+{buffer} Margin')\n",
    "plt.plot([min_val, max_val], [min_val - buffer, max_val - buffer], ls=\"--\", color=\"red\", alpha=0.5, label=f'-{buffer} Margin')\n",
    "\n",
    "plt.xlabel('Actual Performance')\n",
    "plt.ylabel('Predicted Performance')\n",
    "plt.title(\"Predicted vs Actual Performance per Cluster\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f388895f15d094f4",
   "metadata": {},
   "source": [
    "**Comment:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a001ffda699ac9e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eec4b82b5e5666d",
   "metadata": {},
   "source": [
    "### Model 3 :\n",
    "\n",
    "Simple Linear Regression where we impute the missing NAN values using KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e59bccc86d0eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res_method3.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f647c08bbacf6985",
   "metadata": {},
   "source": [
    "**Comment:** ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15498523e66f25a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc6ffa39e02d2931",
   "metadata": {},
   "source": [
    "### Model 4 :\n",
    "\n",
    "Gradient Boosting Regressor that accepts NAN values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c99730a3c6b093",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test R²:\", r2_score(y_test, y_pred))\n",
    "print(\"Test RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "cv_scores = cross_val_score(best_model, X, y, cv=5, scoring='r2', n_jobs=-1)\n",
    "print(\"5‑fold CV R²: %0.3f ± %0.3f\" % (cv_scores.mean(), cv_scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0f980eeabf16b5",
   "metadata": {},
   "source": [
    "**Comment:** ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d670ba47b300c49e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "283a9f2d0fad70ae",
   "metadata": {},
   "source": [
    "### Comparisons between the models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b196ef1e44e38bb",
   "metadata": {},
   "source": [
    "Here compare the results with maybe a visualisation and some analysis ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87461b17deb18905",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_method0.rsquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9644615634ec66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bcecf784e19b66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62968daa",
   "metadata": {},
   "source": [
    "*Your discussion/interpretation about your model's behavior goes here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c94fae199d6da1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a5502b255d30b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d714f7023db7a83e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e49d1dad",
   "metadata": {},
   "source": [
    "## Task 4: Team Reflection\n",
    "Please describe the contributions of each team member to Milestone 4. Reflect on how you worked as team: what went well, what can be improved for the next milestone?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cde86a72",
   "metadata": {},
   "source": [
    "*Your discussion about team responsibilities goes here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c213606db9b9105",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17204e7cd2049de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1a75978792d858",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4dadbfc0498d7171",
   "metadata": {},
   "source": [
    "## Annexe\n",
    "\n",
    "Test without the time dependant activity features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7d5d1360544cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "activity = pd.read_csv('{}/features/activity.csv'.format(DATA_DIR))\n",
    "performances = pd.read_csv('{}/features/performances.csv'.format(DATA_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4aeb9a6fc18dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "performances_math = performances[performances['domain']== 'math'].copy()\n",
    "activity_math = activity[activity['domain']== 'math'].copy()\n",
    "\n",
    "# Rolling window for recent activity\n",
    "rolling_window_days = 10\n",
    "\n",
    "# Convert the 'date' columns to datetime\n",
    "activity_math['activity_updated'] = pd.to_datetime(activity_math['activity_updated'])\n",
    "performances_math['time'] = pd.to_datetime(performances_math['time'])\n",
    "\n",
    "def compute_all_features_for_exam_no_time(exam_row, user_activities, user_exams, window_days=rolling_window_days):\n",
    "\n",
    "    exam_dt = exam_row['time']\n",
    "\n",
    "    # Include activities up to and including exam_date\n",
    "    previous_activities = user_activities[user_activities['activity_updated'] < exam_dt].copy()\n",
    "\n",
    "    # Rolling window (activities in the last N days, including exam day)\n",
    "    window_start = exam_dt - pd.Timedelta(days=window_days)\n",
    "    rolling_activities = previous_activities[previous_activities['activity_updated'] >= window_start].copy()\n",
    "\n",
    "    features = {}\n",
    "\n",
    "    # Recent average time per activity (rolling window)\n",
    "    #total_time_rolling = rolling_activities['time_in_minutes'].sum()\n",
    "    count_rolling = len(rolling_activities)\n",
    "    #features['recent_avg_time_per_activity'] = total_time_rolling / count_rolling if count_rolling > 0 else 0\n",
    "\n",
    "    # Number of days since last activity\n",
    "    if not previous_activities.empty:\n",
    "        last_activity_date = previous_activities['activity_updated'].max()\n",
    "        features['days_since_last_activity'] = (exam_dt - last_activity_date).days\n",
    "    else:\n",
    "        features['days_since_last_activity'] = np.nan\n",
    "\n",
    "    # Total time spent on activities before the exam\n",
    "    #features['total_time_spent_on_activity_before_exam'] = previous_activities['time_in_minutes'].sum() if not previous_activities.empty else 0\n",
    "\n",
    "    # Average percentage on past exams\n",
    "    previous_exams = user_exams[user_exams['time'] < exam_dt]\n",
    "    features['average_performance_past_exams'] = previous_exams['performance'].mean() if not previous_exams.empty else np.nan\n",
    "\n",
    "    # Usage Frequency: Average activities per day in rolling window & Active days ratio\n",
    "    features['avg_activities_per_day_recent'] = count_rolling / window_days if window_days > 0 else np.nan\n",
    "    if not rolling_activities.empty:\n",
    "        distinct_days = rolling_activities['activity_updated'].dt.normalize().nunique()\n",
    "    else:\n",
    "        distinct_days = 0\n",
    "    features['active_days_ratio_recent'] = distinct_days / window_days if window_days > 0 else np.nan\n",
    "\n",
    "    # Activity diversity (rolling window)\n",
    "    features['diversity_recent'] = rolling_activities['activity_type'].nunique() if not rolling_activities.empty else 0\n",
    "\n",
    "\n",
    "    return pd.Series(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da3cd9ca263aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over each exam (grouped by user) in performances_math and compute all features.\n",
    "features_list = []\n",
    "\n",
    "for user_id, user_exams in performances_math.groupby('user_id'):\n",
    "    # Get corresponding activities for the user from activity_math and sort by date\n",
    "    user_activities = activity_math[activity_math['user_id'] == user_id].sort_values('activity_updated')\n",
    "    user_exams_sorted = user_exams.sort_values('time')\n",
    "\n",
    "    for exam_index, exam_row in user_exams_sorted.iterrows():\n",
    "        feats = compute_all_features_for_exam_no_time(exam_row, user_activities, user_exams_sorted, rolling_window_days)\n",
    "        feats['exam_index'] = exam_index\n",
    "        features_list.append(feats)\n",
    "\n",
    "# Output df\n",
    "features_df = pd.DataFrame(features_list).set_index('exam_index')\n",
    "performances_math_features = performances_math.join(features_df, how='left')\n",
    "\n",
    "\n",
    "# scaling the columns\n",
    "columns_to_scale = [ 'days_since_last_activity','average_performance_past_exams','avg_activities_per_day_recent','active_days_ratio_recent','diversity_recent']\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_values = scaler.fit_transform(performances_math_features[columns_to_scale])\n",
    "scaled_df = pd.DataFrame(scaled_values, columns=columns_to_scale, index=performances_math_features.index)\n",
    "remaining_df = performances_math_features.drop(columns=columns_to_scale)\n",
    "final_df = pd.concat([scaled_df, remaining_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccda5bed978a2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Model\n",
    "mod_annexe = smf.ols(\n",
    "    formula='performance ~  days_since_last_activity + average_performance_past_exams + avg_activities_per_day_recent + active_days_ratio_recent + diversity_recent',\n",
    "    data=final_df)\n",
    "\n",
    "# Fit the model\n",
    "res_annexe = mod_annexe.fit()\n",
    "\n",
    "# Print regression results summary\n",
    "print(res_annexe.summary())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "m2-classtime-sciper.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
