{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfb2e10c-b591-4c05-a5fa-cb6ba9568c15",
   "metadata": {},
   "source": [
    "# Extraction of Content Features for the Chat Bot Messages\n",
    "\n",
    "RQ3 : Which features derived from chatbot interactions can help predict whether a student will perform above or below average?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be79d4e1-9fe9-4062-8e01-e3581a2398fc",
   "metadata": {},
   "source": [
    "For our chat bot analysis, we would like to add two content based features:\n",
    "- Emotional Content of User Messages\n",
    "- Kind of Questions they asked, for example: conceptual, homework-specific, procedural ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we translate the text from german to english using the Deepl API. If you want to rerun this process you will have to go and get a free account or two which allows you to get a key!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d604bb-5a30-4b17-93e1-3d1263b4ddf7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import deepl\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Read data that has already been filtered for the 40 percent confidence threshold\n",
    "df = pd.read_csv(\"data/cleaned/gymitrainer_40percent.csv\")\n",
    "\n",
    "# Initialize DeepL\n",
    "auth_key = \"REMOVED\" # removed for github upload \n",
    "translator = deepl.Translator(auth_key)\n",
    "\n",
    "# Step 1: Parse user messages\n",
    "df[\"messages_user\"] = df[\"content\"].apply(lambda x: ast.literal_eval(x)[1::2])\n",
    "\n",
    "# Step 2: Batched + rate-limited DeepL translation\n",
    "def translate_batch_safe(messages, sleep=1.1):\n",
    "    translated = []\n",
    "    for msg in messages:\n",
    "        try:\n",
    "            translated.append(translator.translate_text(msg, source_lang=\"DE\", target_lang=\"EN-US\").text)\n",
    "            time.sleep(sleep)  # Avoid rate limit (50 req/min for free tier)\n",
    "        except Exception as e:\n",
    "            translated.append(\"[TRANSLATION ERROR]\")\n",
    "    return translated\n",
    "\n",
    "tqdm.pandas(desc=\"Translating user messages\")\n",
    "df[\"messages_user_en\"] = df[\"messages_user\"].progress_apply(translate_batch_safe)\n",
    "#df.to_csv(\"translated.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the RoBERTa-based model trained on a combination of publicly available datasets from domains such as Twitter, Reddit, and scripted dialogues. We got this model from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eef2d83e-dd85-463c-b595-b147a46a8b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "  2%|▏         | 58/3500 [00:04<02:43, 21.10it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (5195 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 3500/3500 [02:53<00:00, 20.22it/s]\n",
      "Device set to use mps:0\n",
      "100%|██████████| 3500/3500 [50:48<00:00,  1.15it/s]  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tqdm.auto import tqdm as auto_tqdm\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Enable tqdm integration with pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"translated_retry.csv\")\n",
    "\n",
    "# Clean up messages\n",
    "df[\"messages_user_en\"] = df[\"messages_user_en\"].apply(lambda lst: [m for m in eval(lst) if isinstance(m, str)])\n",
    "\n",
    "# Step 3: Emotion classification\n",
    "emotion_classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"j-hartmann/emotion-english-distilroberta-base\",\n",
    "    return_all_scores=True,\n",
    "    top_k=None,\n",
    "    device=0\n",
    ")\n",
    "\n",
    "emotion_labels = ['anger','disgust','fear','joy','neutral','sadness','surprise']\n",
    "\n",
    "def compute_emotions(msg_list):\n",
    "    if not isinstance(msg_list, list) or not msg_list:\n",
    "        return {f\"avg_{label}\": 0.0 for label in emotion_labels}\n",
    "    try:\n",
    "        scores = emotion_classifier(msg_list)\n",
    "        df_scores = pd.DataFrame([{s['label']: s['score'] for s in msg} for msg in scores])\n",
    "        return df_scores.mean().add_prefix('avg_').to_dict()\n",
    "    except Exception:\n",
    "        return {f\"avg_{label}\": 0.0 for label in emotion_labels}\n",
    "\n",
    "df[\"emotion_results\"] = df[\"messages_user_en\"].progress_apply(compute_emotions)\n",
    "emotion_df = df[\"emotion_results\"].apply(pd.Series)\n",
    "df = pd.concat([df, emotion_df], axis=1)\n",
    "df.to_csv(\"emotions.csv\", index=False)\n",
    "\n",
    "# Step 4: Zero-shot topic classification\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=0)\n",
    "candidate_labels = [\"conceptual\", \"procedural\", \"factual\", \"homework-specific\"]\n",
    "\n",
    "def classify_messages(messages):\n",
    "    if not isinstance(messages, list):\n",
    "        return {label + \"_count\": 0 for label in candidate_labels}\n",
    "    counts = {label + \"_count\": 0 for label in candidate_labels}\n",
    "    try:\n",
    "        results = classifier(messages, candidate_labels)\n",
    "        for result in results:\n",
    "            top = result['labels'][0]\n",
    "            counts[top + \"_count\"] += 1\n",
    "    except Exception:\n",
    "        pass\n",
    "    return counts\n",
    "\n",
    "df[\"classification_counts\"] = df[\"messages_user_en\"].progress_apply(classify_messages)\n",
    "df = pd.concat([df, df[\"classification_counts\"].apply(pd.Series)], axis=1)\n",
    "df.to_csv(\"classifications.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
