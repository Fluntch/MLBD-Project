{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfb2e10c-b591-4c05-a5fa-cb6ba9568c15",
   "metadata": {},
   "source": [
    "# Extraction of Content Features for the Chat Bot Messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be79d4e1-9fe9-4062-8e01-e3581a2398fc",
   "metadata": {},
   "source": [
    "For our chat bot analysis, we would like to add two content based features:\n",
    "- Emotional Content of User Messages\n",
    "- Kind of Questions they asked (Conceptual? Procedural? ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d604bb-5a30-4b17-93e1-3d1263b4ddf7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import deepl\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Read data\n",
    "df = pd.read_csv(\"data/cleaned/gymitrainer_40percent.csv\")\n",
    "\n",
    "# Initialize DeepL\n",
    "auth_key = \"adad831c-ab5c-44a3-a708-609e71f78ad5:fx\"\n",
    "translator = deepl.Translator(auth_key)\n",
    "\n",
    "# Step 1: Parse user messages\n",
    "df[\"messages_user\"] = df[\"content\"].apply(lambda x: ast.literal_eval(x)[1::2])\n",
    "\n",
    "# Step 2: Batched + rate-limited DeepL translation\n",
    "def translate_batch_safe(messages, sleep=1.1):\n",
    "    translated = []\n",
    "    for msg in messages:\n",
    "        try:\n",
    "            translated.append(translator.translate_text(msg, source_lang=\"DE\", target_lang=\"EN-US\").text)\n",
    "            time.sleep(sleep)  # Avoid rate limit (50 req/min for free tier)\n",
    "        except Exception as e:\n",
    "            translated.append(\"[TRANSLATION ERROR]\")\n",
    "    return translated\n",
    "\n",
    "tqdm.pandas(desc=\"Translating user messages\")\n",
    "df[\"messages_user_en\"] = df[\"messages_user\"].progress_apply(translate_batch_safe)\n",
    "df.to_csv(\"translated.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eef2d83e-dd85-463c-b595-b147a46a8b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "  2%|▏         | 58/3500 [00:04<02:43, 21.10it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (5195 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 3500/3500 [02:53<00:00, 20.22it/s]\n",
      "Device set to use mps:0\n",
      "100%|██████████| 3500/3500 [50:48<00:00,  1.15it/s]  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tqdm.auto import tqdm as auto_tqdm\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Enable tqdm integration with pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"translated_retry.csv\")\n",
    "\n",
    "# Clean up messages\n",
    "df[\"messages_user_en\"] = df[\"messages_user_en\"].apply(lambda lst: [m for m in eval(lst) if isinstance(m, str)])\n",
    "\n",
    "# Step 3: Emotion classification\n",
    "emotion_classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"j-hartmann/emotion-english-distilroberta-base\",\n",
    "    return_all_scores=True,\n",
    "    top_k=None,\n",
    "    device=0\n",
    ")\n",
    "\n",
    "emotion_labels = ['anger','disgust','fear','joy','neutral','sadness','surprise']\n",
    "\n",
    "def compute_emotions(msg_list):\n",
    "    if not isinstance(msg_list, list) or not msg_list:\n",
    "        return {f\"avg_{label}\": 0.0 for label in emotion_labels}\n",
    "    try:\n",
    "        scores = emotion_classifier(msg_list)\n",
    "        df_scores = pd.DataFrame([{s['label']: s['score'] for s in msg} for msg in scores])\n",
    "        return df_scores.mean().add_prefix('avg_').to_dict()\n",
    "    except Exception:\n",
    "        return {f\"avg_{label}\": 0.0 for label in emotion_labels}\n",
    "\n",
    "df[\"emotion_results\"] = df[\"messages_user_en\"].progress_apply(compute_emotions)\n",
    "emotion_df = df[\"emotion_results\"].apply(pd.Series)\n",
    "df = pd.concat([df, emotion_df], axis=1)\n",
    "df.to_csv(\"emotions.csv\", index=False)\n",
    "\n",
    "# Step 4: Zero-shot topic classification\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=0)\n",
    "candidate_labels = [\"conceptual\", \"procedural\", \"factual\", \"homework-specific\"]\n",
    "\n",
    "def classify_messages(messages):\n",
    "    if not isinstance(messages, list):\n",
    "        return {label + \"_count\": 0 for label in candidate_labels}\n",
    "    counts = {label + \"_count\": 0 for label in candidate_labels}\n",
    "    try:\n",
    "        results = classifier(messages, candidate_labels)\n",
    "        for result in results:\n",
    "            top = result['labels'][0]\n",
    "            counts[top + \"_count\"] += 1\n",
    "    except Exception:\n",
    "        pass\n",
    "    return counts\n",
    "\n",
    "df[\"classification_counts\"] = df[\"messages_user_en\"].progress_apply(classify_messages)\n",
    "df = pd.concat([df, df[\"classification_counts\"].apply(pd.Series)], axis=1)\n",
    "df.to_csv(\"classifications.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f182a5b-4eae-4324-874f-8a2fc3c6472c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.7.0-cp312-none-macosx_11_0_arm64.whl.metadata (29 kB)\n",
      "Requirement already satisfied: filelock in /Users/fva/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/fva/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /Users/fva/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch) (72.2.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /Users/fva/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/fva/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch) (2025.3.2)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/fva/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Downloading torch-2.7.0-cp312-none-macosx_11_0_arm64.whl (68.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.6/68.6 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, torch\n",
      "Successfully installed mpmath-1.3.0 networkx-3.4.2 sympy-1.14.0 torch-2.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a62759-21f2-4e37-917c-8b7f8c8243a8",
   "metadata": {},
   "source": [
    "For the content analysis and the topic modelling we only are interested in the messages sent by the user. Let's create a column which contains only those ...\n",
    "User messages are every second message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04904f24-9630-4ab9-9f9f-f18aabc979a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()\n",
    "df2 = df2.drop('Unnamed: 0.1', axis=1)\n",
    "df2 = df2.drop('Unnamed: 0', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e6a0f56-abdb-415e-9340-73db426af40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv(\"gogymi_with_content.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a80133d-19a1-40ef-8367-f0a5f82afc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import deepl\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load translated data\n",
    "df = pd.read_csv(\"translated_retry.csv\")\n",
    "\n",
    "# Re-initialize DeepL\n",
    "auth_key = \"cb200b55-d1f4-4483-a1c6-3c8cb05fc576:fx\"\n",
    "translator = deepl.Translator(auth_key)\n",
    "\n",
    "# Identify rows with translation errors\n",
    "mask_error = df[\"messages_user_en\"].apply(lambda lst: \"[TRANSLATION ERROR]\" in lst)\n",
    "df_errors = df[mask_error].copy()\n",
    "\n",
    "# Parse messages_user from string to list if needed\n",
    "if isinstance(df_errors[\"messages_user\"].iloc[0], str):\n",
    "    import ast\n",
    "    df_errors[\"messages_user\"] = df_errors[\"messages_user\"].apply(ast.literal_eval)\n",
    "\n",
    "# Retry translation\n",
    "def retry_translation(messages):\n",
    "    results = []\n",
    "    for msg in messages:\n",
    "        if not msg.strip():\n",
    "            results.append(\"\")  # Skip empty messages\n",
    "            continue\n",
    "        if msg == \"[TRANSLATION ERROR]\":\n",
    "            results.append(\"[TRANSLATION ERROR]\")  # Already marked as error\n",
    "            continue\n",
    "        try:\n",
    "            translated = translator.translate_text(msg, source_lang=\"DE\", target_lang=\"EN-US\").text\n",
    "            results.append(translated)\n",
    "        except Exception as e:\n",
    "            print(\"⚠️ Translation error occurred. Waiting 30 seconds before retrying...\")\n",
    "            print(e)\n",
    "            time.sleep(30)\n",
    "            try:\n",
    "                results.append(translated)\n",
    "            except Exception:\n",
    "                results.append(\"[TRANSLATION ERROR]\")\n",
    "    return results\n",
    "\n",
    "# Progressively retry and save\n",
    "output_path = \"translated_retry.csv\"\n",
    "save_interval = 50\n",
    "\n",
    "pbar = tqdm(total=len(df_errors), desc=\"Retrying failed translations\")\n",
    "for idx, (i, row) in enumerate(df_errors.iterrows()):\n",
    "    df_errors.at[i, \"messages_user_en\"] = retry_translation(row[\"messages_user\"])\n",
    "\n",
    "    if (idx + 1) % save_interval == 0 or idx == len(df_errors) - 1:\n",
    "        df.update(df_errors)\n",
    "        df.to_csv(output_path, index=False)\n",
    "        pbar.set_postfix(saved_rows=idx + 1)\n",
    "\n",
    "    pbar.update(1)\n",
    "\n",
    "pbar.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
