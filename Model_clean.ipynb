{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-24T18:05:44.693905Z",
     "start_time": "2025-04-24T18:05:44.692495Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T18:05:44.788272Z",
     "start_time": "2025-04-24T18:05:44.698551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "activity = pd.read_csv('data/features/activity.csv')\n",
    "performances = pd.read_csv('data/features/performances.csv')"
   ],
   "id": "c443fa3f4ee6995",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T18:05:45.225486Z",
     "start_time": "2025-04-24T18:05:45.215133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "performances_math = performances[performances['domain']== 'math'].copy()\n",
    "activity_math = activity[activity['domain']== 'math'].copy()"
   ],
   "id": "cce5a47e82078486",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T18:05:45.538524Z",
     "start_time": "2025-04-24T18:05:45.533752Z"
    }
   },
   "cell_type": "code",
   "source": "len(activity_math)",
   "id": "4659386361ea529a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25938"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T18:05:45.843853Z",
     "start_time": "2025-04-24T18:05:45.834415Z"
    }
   },
   "cell_type": "code",
   "source": "activity_math.isnull().sum()",
   "id": "129c90ee1ac5b929",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "activity_id               0\n",
       "user_id                   0\n",
       "post_id                   0\n",
       "course_id                 0\n",
       "activity_type             0\n",
       "activity_status           0\n",
       "activity_started          0\n",
       "activity_completed    12676\n",
       "activity_updated          0\n",
       "domain                    0\n",
       "date_restored             0\n",
       "times_valid               0\n",
       "date                      0\n",
       "time_spent                0\n",
       "time_in_minutes           0\n",
       "time_truncated            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T18:05:46.353291Z",
     "start_time": "2025-04-24T18:05:46.343777Z"
    }
   },
   "cell_type": "code",
   "source": "activity_math.dropna(inplace=True)",
   "id": "7dc9f3e344a0bd4c",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can see that almost 50% of the rows contain nans in the activity_completed column. Unfortunately we have to drop all these rows for our analysis because 2 out of our 7 features depend on the time spent on the activities which we cannot compute without the activity completed value. (we can see in the \"Annex : Test without the time dependant activity features\" section that keeping all the data and removing those 2 time dependant features makes our model worst which confirms our choice)",
   "id": "ffd990be8314818c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Feature extraction",
   "id": "16f3d4413ba8040e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T18:05:50.634255Z",
     "start_time": "2025-04-24T18:05:47.525974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Rolling window for recent activity\n",
    "rolling_window_days = 10\n",
    "\n",
    "# Convert the 'date' columns to datetime\n",
    "activity_math['activity_updated'] = pd.to_datetime(activity_math['activity_updated'])\n",
    "performances_math['time'] = pd.to_datetime(performances_math['time'])\n",
    "\n",
    "def compute_all_features_for_exam(exam_row, user_activities, user_exams, window_days=rolling_window_days):\n",
    "\n",
    "    exam_dt = exam_row['time']\n",
    "\n",
    "    # Include activities up to and including exam_date\n",
    "    previous_activities = user_activities[user_activities['activity_updated'] < exam_dt].copy()\n",
    "\n",
    "    # Rolling window (activities in the last N days, including exam day)\n",
    "    window_start = exam_dt - pd.Timedelta(days=window_days)\n",
    "    rolling_activities = previous_activities[previous_activities['activity_updated'] >= window_start].copy()\n",
    "\n",
    "    features = {}\n",
    "\n",
    "    # Recent average time per activity (rolling window)\n",
    "    total_time_rolling = rolling_activities['time_in_minutes'].sum()\n",
    "    count_rolling = len(rolling_activities)\n",
    "    features['recent_avg_time_per_activity'] = total_time_rolling / count_rolling if count_rolling > 0 else 0\n",
    "\n",
    "    # Number of days since last activity\n",
    "    if not previous_activities.empty:\n",
    "        last_activity_date = previous_activities['activity_updated'].max()\n",
    "        features['days_since_last_activity'] = (exam_dt - last_activity_date).days\n",
    "    else:\n",
    "        features['days_since_last_activity'] = np.nan\n",
    "\n",
    "    # Total time spent on activities before the exam\n",
    "    features['total_time_spent_on_activity_before_exam'] = previous_activities['time_in_minutes'].sum() if not previous_activities.empty else 0\n",
    "\n",
    "    # Average percentage on past exams\n",
    "    previous_exams = user_exams[user_exams['time'] < exam_dt]\n",
    "    features['average_performance_past_exams'] = previous_exams['performance'].mean() if not previous_exams.empty else np.nan\n",
    "\n",
    "    # Usage Frequency: Average activities per day in rolling window & Active days ratio\n",
    "    features['avg_activities_per_day_recent'] = count_rolling / window_days if window_days > 0 else np.nan\n",
    "    if not rolling_activities.empty:\n",
    "        distinct_days = rolling_activities['activity_updated'].dt.normalize().nunique()\n",
    "    else:\n",
    "        distinct_days = 0\n",
    "    features['active_days_ratio_recent'] = distinct_days / window_days if window_days > 0 else np.nan\n",
    "\n",
    "    # Activity diversity (rolling window)\n",
    "    features['diversity_recent'] = rolling_activities['activity_type'].nunique() if not rolling_activities.empty else 0\n",
    "\n",
    "\n",
    "    return pd.Series(features)\n",
    "\n",
    "# Loop over each exam (grouped by user) in performances_math and compute all features.\n",
    "features_list = []\n",
    "\n",
    "for user_id, user_exams in performances_math.groupby('user_id'):\n",
    "    # Get corresponding activities for the user from activity_math and sort by date\n",
    "    user_activities = activity_math[activity_math['user_id'] == user_id].sort_values('activity_updated')\n",
    "    user_exams_sorted = user_exams.sort_values('time')\n",
    "\n",
    "    for exam_index, exam_row in user_exams_sorted.iterrows():\n",
    "        feats = compute_all_features_for_exam(exam_row, user_activities, user_exams_sorted, rolling_window_days)\n",
    "        feats['exam_index'] = exam_index\n",
    "        features_list.append(feats)\n",
    "\n",
    "# Output df\n",
    "features_df = pd.DataFrame(features_list).set_index('exam_index')\n",
    "performances_math_features = performances_math.join(features_df, how='left')"
   ],
   "id": "9534bd1f49b2ca85",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T18:05:51.071409Z",
     "start_time": "2025-04-24T18:05:51.067355Z"
    }
   },
   "cell_type": "code",
   "source": "performances_math_features.isnull().sum()",
   "id": "b0382e6a19e12f12",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id                                       0\n",
       "domain                                        0\n",
       "test_id                                       0\n",
       "course                                        0\n",
       "date                                          0\n",
       "time                                          0\n",
       "percentage                                    0\n",
       "performance                                   0\n",
       "recent_avg_time_per_activity                  0\n",
       "days_since_last_activity                     56\n",
       "total_time_spent_on_activity_before_exam      0\n",
       "average_performance_past_exams              469\n",
       "avg_activities_per_day_recent                 0\n",
       "active_days_ratio_recent                      0\n",
       "diversity_recent                              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can see that we have quite a lot of Nans in the average_percentage_past_exams column, Those correspond to the first exam questions that every user did for which there is no past value. We will first try to train a linear model by dropping those rows and then we will try to impute these values with the average exam percentage of similar users and see which model performs best.",
   "id": "26ccee8677fe08b9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model where we just drop all rows containing Nans",
   "id": "ab8514d9298e008"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T18:05:52.066169Z",
     "start_time": "2025-04-24T18:05:52.048646Z"
    }
   },
   "cell_type": "code",
   "source": "performances_math_features_drop = performances_math_features.dropna()",
   "id": "7a99303ed292042e",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T18:05:52.771202Z",
     "start_time": "2025-04-24T18:05:52.673655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# scaling the columns\n",
    "columns_to_scale = ['recent_avg_time_per_activity', 'days_since_last_activity', 'total_time_spent_on_activity_before_exam','average_performance_past_exams','avg_activities_per_day_recent','active_days_ratio_recent','diversity_recent']\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_values = scaler.fit_transform(performances_math_features_drop[columns_to_scale])\n",
    "scaled_df = pd.DataFrame(scaled_values, columns=columns_to_scale, index=performances_math_features_drop.index)\n",
    "remaining_df = performances_math_features_drop.drop(columns=columns_to_scale)\n",
    "final_df_drop = pd.concat([scaled_df, remaining_df], axis=1)"
   ],
   "id": "5da7eeb525c48a",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T18:05:53.445194Z",
     "start_time": "2025-04-24T18:05:53.363349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Linear Regression Model\n",
    "mod = smf.ols(formula= 'performance ~  recent_avg_time_per_activity + days_since_last_activity + total_time_spent_on_activity_before_exam + average_performance_past_exams + avg_activities_per_day_recent + active_days_ratio_recent + diversity_recent', data=final_df_drop)\n",
    "\n",
    "# Fit the model\n",
    "res = mod.fit()\n",
    "\n",
    "# Print regression results summary\n",
    "print(res.summary())"
   ],
   "id": "b2d0dd0e51b0e0d8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:            performance   R-squared:                       0.267\n",
      "Model:                            OLS   Adj. R-squared:                  0.266\n",
      "Method:                 Least Squares   F-statistic:                     173.6\n",
      "Date:                Thu, 24 Apr 2025   Prob (F-statistic):          1.25e-219\n",
      "Time:                        20:05:53   Log-Likelihood:                -15421.\n",
      "No. Observations:                3340   AIC:                         3.086e+04\n",
      "Df Residuals:                    3332   BIC:                         3.091e+04\n",
      "Df Model:                           7                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "============================================================================================================\n",
      "                                               coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Intercept                                    0.3219      0.424      0.759      0.448      -0.510       1.154\n",
      "recent_avg_time_per_activity                 1.9646      0.468      4.202      0.000       1.048       2.881\n",
      "days_since_last_activity                     1.3947      0.525      2.656      0.008       0.365       2.424\n",
      "total_time_spent_on_activity_before_exam     1.4687      0.463      3.172      0.002       0.561       2.377\n",
      "average_performance_past_exams              13.0339      0.473     27.579      0.000      12.107      13.961\n",
      "avg_activities_per_day_recent               -1.6621      0.550     -3.024      0.003      -2.740      -0.584\n",
      "active_days_ratio_recent                     0.8565      0.589      1.453      0.146      -0.299       2.012\n",
      "diversity_recent                            -0.1706      0.617     -0.276      0.782      -1.381       1.040\n",
      "==============================================================================\n",
      "Omnibus:                       26.731   Durbin-Watson:                   1.667\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               18.529\n",
      "Skew:                          -0.042   Prob(JB):                     9.47e-05\n",
      "Kurtosis:                       2.645   Cond. No.                         2.71\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We get an R^2 value of 0.267 and can see that the average percentage on last exam seems to be the most important feature out of the lot by quite a margin lets thus try to impute the missing average_percentage_past_exams values missing to see if we can improve our model. we also saw that using the average performance on last exams intead of the avergae percenatge on last exams improves the model quite a bit.... TO DEVELOPP MORE",
   "id": "3e3b6403b2abdb5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Now lets try to impute the misssing average_perfomance_last_exam_values",
   "id": "b61cea9e2a698ec6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T18:05:55.099578Z",
     "start_time": "2025-04-24T18:05:55.095261Z"
    }
   },
   "cell_type": "code",
   "source": "performance_math_impute = performances_math_features.copy()",
   "id": "633bbc569aee9d30",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T18:05:56.863653Z",
     "start_time": "2025-04-24T18:05:56.597971Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Features we’ll use to compute similarity:\n",
    "sim_features = ['recent_avg_time_per_activity','days_since_last_activity','total_time_spent_on_activity_before_exam',\n",
    "                'avg_activities_per_day_recent','active_days_ratio_recent','diversity_recent']\n",
    "\n",
    "# New column to hold the imputed values\n",
    "performance_math_impute['avg_perf_past_exams_imputed'] = performance_math_impute['average_performance_past_exams']\n",
    "\n",
    "# Group by test_id and run kNN inside each group\n",
    "for test_id, group in performance_math_impute.groupby('test_id'):\n",
    "    # indices of rows we need to fill\n",
    "    missing_idx = group[group['average_performance_past_exams'].isna()].index\n",
    "    if len(missing_idx) == 0:\n",
    "        continue\n",
    "\n",
    "    # candidate neighbors: same test, non‐missing avg_pct\n",
    "    candidates = group[group['average_performance_past_exams'].notna()]\n",
    "    if candidates.shape[0] == 0:\n",
    "        # if no one else took that test --> impute median\n",
    "        continue\n",
    "\n",
    "    # Matrix of sim_features, impute median for remaining NaNs\n",
    "    feat_mat = group[sim_features].copy()\n",
    "    feat_mat = feat_mat.fillna(feat_mat.median())\n",
    "\n",
    "    # Split into X_train (candidates) and X_query (the missing rows)\n",
    "    X_train = feat_mat.loc[candidates.index].values\n",
    "    X_query = feat_mat.loc[missing_idx].values\n",
    "\n",
    "    # Use up to 3 neighbors (fewer if not enough candidates)\n",
    "    k = min(3, X_train.shape[0])\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, algorithm='auto').fit(X_train)\n",
    "    distances, neighbors = nbrs.kneighbors(X_query)\n",
    "\n",
    "    # For each missing row, average the actual test scores of its neighbors\n",
    "    candidate_scores = candidates['performance'].values\n",
    "    for i, idx in enumerate(missing_idx):\n",
    "        nbr_idxs = neighbors[i]\n",
    "        imputed_val = candidate_scores[nbr_idxs].mean()\n",
    "        performance_math_impute.at[idx, 'avg_perf_past_exams_imputed'] = imputed_val\n",
    "\n",
    "# Replace the original column\n",
    "performance_math_impute['average_performance_past_exams'] = performance_math_impute['avg_perf_past_exams_imputed']\n",
    "performance_math_impute.drop(columns='avg_perf_past_exams_imputed', inplace=True)"
   ],
   "id": "3fb49300c270f086",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T18:05:57.463341Z",
     "start_time": "2025-04-24T18:05:57.455381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# scaling the columns\n",
    "columns_to_scale = ['recent_avg_time_per_activity', 'days_since_last_activity', 'total_time_spent_on_activity_before_exam','average_performance_past_exams','avg_activities_per_day_recent','active_days_ratio_recent','diversity_recent']\n",
    "scaler = StandardScaler()\n",
    "scaled_values = scaler.fit_transform(performance_math_impute[columns_to_scale])\n",
    "scaled_df = pd.DataFrame(scaled_values, columns=columns_to_scale, index=performance_math_impute.index)\n",
    "remaining_df = performance_math_impute.drop(columns=columns_to_scale)\n",
    "final_df_impute = pd.concat([scaled_df, remaining_df], axis=1)"
   ],
   "id": "81b3e5c5e5118ce0",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T18:05:58.188588Z",
     "start_time": "2025-04-24T18:05:58.183950Z"
    }
   },
   "cell_type": "code",
   "source": "final_df_impute.dropna(inplace=True) # drop the remaining NaNs",
   "id": "4eee2dd03a107793",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T18:05:58.956749Z",
     "start_time": "2025-04-24T18:05:58.936016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Linear Regression Model\n",
    "mod = smf.ols(formula= 'performance ~  recent_avg_time_per_activity + days_since_last_activity + total_time_spent_on_activity_before_exam + average_performance_past_exams + avg_activities_per_day_recent + active_days_ratio_recent + diversity_recent', data=final_df_impute)\n",
    "\n",
    "# Fit the model\n",
    "res = mod.fit()\n",
    "\n",
    "# Print regression results summary\n",
    "print(res.summary())"
   ],
   "id": "d33cc8a2d25a4e1f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:            performance   R-squared:                       0.240\n",
      "Model:                            OLS   Adj. R-squared:                  0.238\n",
      "Method:                 Least Squares   F-statistic:                     168.8\n",
      "Date:                Thu, 24 Apr 2025   Prob (F-statistic):          1.34e-217\n",
      "Time:                        20:05:58   Log-Likelihood:                -17353.\n",
      "No. Observations:                3754   AIC:                         3.472e+04\n",
      "Df Residuals:                    3746   BIC:                         3.477e+04\n",
      "Df Model:                           7                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "============================================================================================================\n",
      "                                               coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Intercept                                   -0.2131      0.403     -0.529      0.597      -1.002       0.576\n",
      "recent_avg_time_per_activity                 1.9006      0.445      4.274      0.000       1.029       2.772\n",
      "days_since_last_activity                     1.2817      0.496      2.584      0.010       0.309       2.254\n",
      "total_time_spent_on_activity_before_exam     1.6790      0.444      3.778      0.000       0.808       2.550\n",
      "average_performance_past_exams              12.0838      0.447     27.009      0.000      11.207      12.961\n",
      "avg_activities_per_day_recent               -1.8492      0.521     -3.547      0.000      -2.871      -0.827\n",
      "active_days_ratio_recent                     0.5352      0.569      0.940      0.347      -0.581       1.652\n",
      "diversity_recent                             0.3069      0.594      0.517      0.606      -0.858       1.472\n",
      "==============================================================================\n",
      "Omnibus:                       38.123   Durbin-Watson:                   1.720\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               25.414\n",
      "Skew:                          -0.055   Prob(JB):                     3.03e-06\n",
      "Kurtosis:                       2.612   Cond. No.                         2.75\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can see that imputing the values for the first exam of each student actually make the model worst (R^2 of 0.240) --> it is better to just drop the rows.\n",
   "id": "8c3ae023cf1c2510"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### let's now see if simply impute with the median of that exam",
   "id": "7590a0c6e62c650c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T18:06:01.826695Z",
     "start_time": "2025-04-24T18:06:01.821485Z"
    }
   },
   "cell_type": "code",
   "source": "performance_math_median = performances_math_features.copy()",
   "id": "bffc22b969c69cb3",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T18:06:20.413698Z",
     "start_time": "2025-04-24T18:06:20.380210Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Fill by test_id median\n",
    "performance_math_median['average_performance_past_exams'] = (\n",
    "    performance_math_median\n",
    "    .groupby('test_id')['average_performance_past_exams']\n",
    "    .transform(lambda x: x.fillna(x.median()))\n",
    "    .fillna(performance_math_median['average_performance_past_exams'].median())   # fallback to global median if a test has no median\n",
    ")"
   ],
   "id": "a2b9479d3797cbf9",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T18:06:21.482774Z",
     "start_time": "2025-04-24T18:06:21.473326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# scaling the columns\n",
    "columns_to_scale = ['recent_avg_time_per_activity', 'days_since_last_activity', 'total_time_spent_on_activity_before_exam','average_performance_past_exams','avg_activities_per_day_recent','active_days_ratio_recent','diversity_recent']\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_values = scaler.fit_transform(performance_math_median[columns_to_scale])\n",
    "scaled_df = pd.DataFrame(scaled_values, columns=columns_to_scale, index=performance_math_median.index)\n",
    "remaining_df = performance_math_median.drop(columns=columns_to_scale)\n",
    "final_df_med = pd.concat([scaled_df, remaining_df], axis=1)"
   ],
   "id": "4ff09f83e819ad49",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T18:06:24.833878Z",
     "start_time": "2025-04-24T18:06:24.810700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Linear Regression Model\n",
    "mod = smf.ols(formula= 'performance ~  recent_avg_time_per_activity + days_since_last_activity + total_time_spent_on_activity_before_exam + average_performance_past_exams + avg_activities_per_day_recent + active_days_ratio_recent + diversity_recent', data=final_df_med)\n",
    "\n",
    "# Fit the model\n",
    "res = mod.fit()\n",
    "\n",
    "# Print regression results summary\n",
    "print(res.summary())"
   ],
   "id": "70fde05e8368b373",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:            performance   R-squared:                       0.242\n",
      "Model:                            OLS   Adj. R-squared:                  0.240\n",
      "Method:                 Least Squares   F-statistic:                     170.6\n",
      "Date:                Thu, 24 Apr 2025   Prob (F-statistic):          9.72e-220\n",
      "Time:                        20:06:24   Log-Likelihood:                -17349.\n",
      "No. Observations:                3754   AIC:                         3.471e+04\n",
      "Df Residuals:                    3746   BIC:                         3.476e+04\n",
      "Df Model:                           7                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "============================================================================================================\n",
      "                                               coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Intercept                                   -0.1106      0.402     -0.275      0.783      -0.899       0.678\n",
      "recent_avg_time_per_activity                 1.9716      0.443      4.446      0.000       1.102       2.841\n",
      "days_since_last_activity                     1.3458      0.495      2.718      0.007       0.375       2.317\n",
      "total_time_spent_on_activity_before_exam     1.7889      0.443      4.042      0.000       0.921       2.657\n",
      "average_performance_past_exams              12.0288      0.442     27.228      0.000      11.163      12.895\n",
      "avg_activities_per_day_recent               -1.6891      0.521     -3.240      0.001      -2.711      -0.667\n",
      "active_days_ratio_recent                     0.5081      0.569      0.893      0.372      -0.607       1.623\n",
      "diversity_recent                             0.3030      0.593      0.511      0.610      -0.860       1.466\n",
      "==============================================================================\n",
      "Omnibus:                       36.328   Durbin-Watson:                   1.735\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               24.777\n",
      "Skew:                          -0.063   Prob(JB):                     4.17e-06\n",
      "Kurtosis:                       2.623   Cond. No.                         2.75\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "doesn't improve the model either...",
   "id": "48bc9dbd216b695c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Let's now see if another model that supports Nans performs better than simple linear regression",
   "id": "c824443c24fd8088"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T17:13:29.379953Z",
     "start_time": "2025-04-24T17:13:21.299322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "\n",
    "df = performances_math_features.copy()\n",
    "\n",
    "feature_cols = ['recent_avg_time_per_activity','days_since_last_activity','total_time_spent_on_activity_before_exam','average_performance_past_exams',\n",
    "    'avg_activities_per_day_recent','active_days_ratio_recent','diversity_recent']\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df['performance']\n",
    "\n",
    "# Train-Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Pipeline: impute → scale → gradient boost\n",
    "pipeline = Pipeline([\n",
    "    ('imputer',   SimpleImputer(strategy='median')),\n",
    "    ('scaler',    StandardScaler()),\n",
    "    ('gbr',       GradientBoostingRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Grid search for key hyperparameters\n",
    "param_grid = {'gbr__n_estimators': [100, 200], 'gbr__learning_rate': [0.05, 0.1], 'gbr__max_depth':[3, 5], 'gbr__subsample':[0.8, 1.0]}\n",
    "grid = GridSearchCV(pipeline, param_grid, cv=5, scoring='r2', n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit the model\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"Test R²:\", r2_score(y_test, y_pred))\n",
    "print(\"Test RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "# Cross‑validated performance\n",
    "cv_scores = cross_val_score(best_model, X, y, cv=5, scoring='r2', n_jobs=-1)\n",
    "print(\"5‑fold CV R²: %0.3f ± %0.3f\" % (cv_scores.mean(), cv_scores.std()))\n"
   ],
   "id": "3507c7e587fc0b22",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "Best params: {'gbr__learning_rate': 0.05, 'gbr__max_depth': 3, 'gbr__n_estimators': 100, 'gbr__subsample': 0.8}\n",
      "Test R²: 0.2753500683990725\n",
      "Test RMSE: 23.89582817994709\n",
      "5‑fold CV R²: 0.217 ± 0.081\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The original R^2 on the test set is better than our linear model but with the cross validation we can see that it is actually less good on average. NEED TO SEE OTHER EVALUATION CRITERIA",
   "id": "e0fecbcbe990ffd7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Let's see if the model improves if we first cluster the students and then train a mixed effects model",
   "id": "1da01d5545d44ccd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T17:50:38.116903Z",
     "start_time": "2025-04-24T17:50:37.961016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Collapse to one row per user for clustering\n",
    "user_feats = final_df_drop.groupby('user_id')[['recent_avg_time_per_activity', 'days_since_last_activity', 'total_time_spent_on_activity_before_exam', 'average_performance_past_exams', 'avg_activities_per_day_recent', 'active_days_ratio_recent', 'diversity_recent']].mean().reset_index()\n",
    "\n",
    "# Cluster into 3 groups\n",
    "kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "user_feats['cluster'] = kmeans.fit_predict(user_feats.drop(columns='user_id'))\n",
    "\n",
    "# Merge cluster label back into final_df\n",
    "df2 = final_df_drop.merge(user_feats[['user_id','cluster']], on='user_id')\n",
    "\n",
    "# 4) Fit a mixed‐effects model with random intercept per cluster\n",
    "md = smf.mixedlm(\"performance ~ recent_avg_time_per_activity + days_since_last_activity + \\\n",
    "     total_time_spent_on_activity_before_exam + average_performance_past_exams + \\\n",
    "     avg_activities_per_day_recent + active_days_ratio_recent + diversity_recent\",df2,groups=\"cluster\")\n",
    "mdf = md.fit(reml=False)\n",
    "print(mdf.summary())\n",
    "\n",
    "# In-sample predictions\n",
    "df2[\"pred_cluster\"] = mdf.predict(df2)\n",
    "\n",
    "# Compute R² and RMSE\n",
    "r2_cluster = r2_score(df2[\"performance\"], df2[\"pred_cluster\"])\n",
    "rmse_cluster = mean_squared_error(df2[\"performance\"], df2[\"pred_cluster\"])\n",
    "\n",
    "print(\"Cluster‐model  AIC:\",  mdf.aic, \"  BIC:\", mdf.bic)\n",
    "print(f\"Cluster‐model  R² = {r2_cluster:.3f},  RMSE = {rmse_cluster:.3f}\")"
   ],
   "id": "c32274262706de4e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Mixed Linear Model Regression Results\n",
      "===================================================================================\n",
      "Model:                      MixedLM         Dependent Variable:         performance\n",
      "No. Observations:           3340            Method:                     ML         \n",
      "No. Groups:                 3               Scale:                      590.8551   \n",
      "Min. group size:            15              Log-Likelihood:             -15400.5039\n",
      "Max. group size:            1806            Converged:                  No         \n",
      "Mean group size:            1113.3                                                 \n",
      "-----------------------------------------------------------------------------------\n",
      "                                         Coef.  Std.Err.   z    P>|z| [0.025 0.975]\n",
      "-----------------------------------------------------------------------------------\n",
      "Intercept                                 0.126    2.762  0.046 0.964 -5.287  5.539\n",
      "recent_avg_time_per_activity              1.670    0.466  3.582 0.000  0.756  2.584\n",
      "days_since_last_activity                  1.264    0.548  2.306 0.021  0.190  2.338\n",
      "total_time_spent_on_activity_before_exam  0.937    0.466  2.008 0.045  0.022  1.851\n",
      "average_performance_past_exams           11.023    0.557 19.781 0.000  9.931 12.116\n",
      "avg_activities_per_day_recent            -2.320    0.554 -4.188 0.000 -3.406 -1.234\n",
      "active_days_ratio_recent                  0.462    0.588  0.786 0.432 -0.690  1.614\n",
      "diversity_recent                         -0.512    0.617 -0.830 0.406 -1.720  0.697\n",
      "cluster Var                              16.434    0.757                           \n",
      "===================================================================================\n",
      "\n",
      "Cluster‐model  AIC: 30821.007834053995   BIC: 30882.145094913703\n",
      "Cluster‐model  R² = 0.259,  RMSE = 606.159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ada_project/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/opt/anaconda3/envs/ada_project/lib/python3.12/site-packages/statsmodels/regression/mixed_linear_model.py:2200: ConvergenceWarning: Retrying MixedLM optimization with lbfgs\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/ada_project/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/opt/anaconda3/envs/ada_project/lib/python3.12/site-packages/statsmodels/regression/mixed_linear_model.py:2200: ConvergenceWarning: Retrying MixedLM optimization with cg\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/ada_project/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/opt/anaconda3/envs/ada_project/lib/python3.12/site-packages/statsmodels/regression/mixed_linear_model.py:2206: ConvergenceWarning: MixedLM optimization failed, trying a different optimizer may help.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n",
      "/opt/anaconda3/envs/ada_project/lib/python3.12/site-packages/statsmodels/regression/mixed_linear_model.py:2218: ConvergenceWarning: Gradient optimization failed, |grad| = 1.856349\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Bet results with 3 clusters but doesn't actually improve the model compare to the simple linear model. NEED ALSO TO ANALYSE THE AIC AND BICs for each model + Maybe add some viz",
   "id": "235181243ab2e4d2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Annexe : Test without the time dependant activity features",
   "id": "f02a27eb11e34336"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T15:31:29.349544Z",
     "start_time": "2025-04-24T15:31:29.252591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "activity = pd.read_csv('data/features/activity.csv')\n",
    "performances = pd.read_csv('data/features/performances.csv')"
   ],
   "id": "6bbf526c7997d982",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T15:31:29.641376Z",
     "start_time": "2025-04-24T15:31:29.629265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "performances_math = performances[performances['domain']== 'math'].copy()\n",
    "activity_math = activity[activity['domain']== 'math'].copy()"
   ],
   "id": "a60d4df5a0040816",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T15:31:30.114656Z",
     "start_time": "2025-04-24T15:31:30.106325Z"
    }
   },
   "cell_type": "code",
   "source": "activity_math",
   "id": "4888a3c1d9f2573a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       activity_id  user_id  post_id  course_id activity_type  \\\n",
       "0             1128     2533       42         42        course   \n",
       "1             1129     2533       55         42        lesson   \n",
       "2             1130     2533       98         42         topic   \n",
       "3             1131     2533      100         42         topic   \n",
       "4             1132     2533      102         42         topic   \n",
       "...            ...      ...      ...        ...           ...   \n",
       "50137       114742      955      647         42         topic   \n",
       "50138       114743      955      106         42         topic   \n",
       "50139       114744      955      104         42         topic   \n",
       "50140       114745      955      108         42         topic   \n",
       "50141       114746      955     2945         42          quiz   \n",
       "\n",
       "       activity_status     activity_started   activity_completed  \\\n",
       "0                    0  2023-04-07 16:42:35  2023-04-07 17:35:15   \n",
       "1                    0  2023-04-07 16:42:35                  NaN   \n",
       "2                    1  2023-04-07 16:42:38  2023-04-07 16:43:58   \n",
       "3                    1  2023-04-07 16:43:59  2023-04-07 16:46:13   \n",
       "4                    1  2023-04-07 16:46:14  2023-04-07 16:46:27   \n",
       "...                ...                  ...                  ...   \n",
       "50137                0  2025-03-07 06:36:32                  NaN   \n",
       "50138                0  2025-03-07 06:37:49                  NaN   \n",
       "50139                0  2025-03-07 06:37:50                  NaN   \n",
       "50140                0  2025-03-07 06:37:54                  NaN   \n",
       "50141                1  2025-03-07 06:42:09  2025-03-07 06:54:21   \n",
       "\n",
       "          activity_updated domain  date_restored  times_valid        date  \\\n",
       "0      2023-04-07 17:35:15   math           True         True  2023-04-07   \n",
       "1      2023-04-07 16:42:35   math          False         True  2023-04-07   \n",
       "2      2023-04-07 16:43:58   math          False         True  2023-04-07   \n",
       "3      2023-04-07 16:46:13   math          False         True  2023-04-07   \n",
       "4      2023-04-07 16:46:27   math          False         True  2023-04-07   \n",
       "...                    ...    ...            ...          ...         ...   \n",
       "50137  2025-03-07 06:36:32   math          False         True  2025-03-07   \n",
       "50138  2025-03-07 06:37:49   math          False         True  2025-03-07   \n",
       "50139  2025-03-07 06:37:50   math          False         True  2025-03-07   \n",
       "50140  2025-03-07 06:37:54   math          False         True  2025-03-07   \n",
       "50141  2025-03-07 06:54:21   math          False         True  2025-03-07   \n",
       "\n",
       "            time_spent  time_in_minutes  time_truncated  \n",
       "0      0 days 00:52:40        52.666667           False  \n",
       "1      0 days 00:00:00         0.000000           False  \n",
       "2      0 days 00:01:20         1.333333           False  \n",
       "3      0 days 00:02:14         2.233333           False  \n",
       "4      0 days 00:00:13         0.216667           False  \n",
       "...                ...              ...             ...  \n",
       "50137  0 days 00:00:00         0.000000           False  \n",
       "50138  0 days 00:00:00         0.000000           False  \n",
       "50139  0 days 00:00:00         0.000000           False  \n",
       "50140  0 days 00:00:00         0.000000           False  \n",
       "50141  0 days 00:12:12        12.200000           False  \n",
       "\n",
       "[25938 rows x 16 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activity_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>post_id</th>\n",
       "      <th>course_id</th>\n",
       "      <th>activity_type</th>\n",
       "      <th>activity_status</th>\n",
       "      <th>activity_started</th>\n",
       "      <th>activity_completed</th>\n",
       "      <th>activity_updated</th>\n",
       "      <th>domain</th>\n",
       "      <th>date_restored</th>\n",
       "      <th>times_valid</th>\n",
       "      <th>date</th>\n",
       "      <th>time_spent</th>\n",
       "      <th>time_in_minutes</th>\n",
       "      <th>time_truncated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1128</td>\n",
       "      <td>2533</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>course</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-04-07 16:42:35</td>\n",
       "      <td>2023-04-07 17:35:15</td>\n",
       "      <td>2023-04-07 17:35:15</td>\n",
       "      <td>math</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2023-04-07</td>\n",
       "      <td>0 days 00:52:40</td>\n",
       "      <td>52.666667</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1129</td>\n",
       "      <td>2533</td>\n",
       "      <td>55</td>\n",
       "      <td>42</td>\n",
       "      <td>lesson</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-04-07 16:42:35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-07 16:42:35</td>\n",
       "      <td>math</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2023-04-07</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1130</td>\n",
       "      <td>2533</td>\n",
       "      <td>98</td>\n",
       "      <td>42</td>\n",
       "      <td>topic</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-04-07 16:42:38</td>\n",
       "      <td>2023-04-07 16:43:58</td>\n",
       "      <td>2023-04-07 16:43:58</td>\n",
       "      <td>math</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2023-04-07</td>\n",
       "      <td>0 days 00:01:20</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1131</td>\n",
       "      <td>2533</td>\n",
       "      <td>100</td>\n",
       "      <td>42</td>\n",
       "      <td>topic</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-04-07 16:43:59</td>\n",
       "      <td>2023-04-07 16:46:13</td>\n",
       "      <td>2023-04-07 16:46:13</td>\n",
       "      <td>math</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2023-04-07</td>\n",
       "      <td>0 days 00:02:14</td>\n",
       "      <td>2.233333</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1132</td>\n",
       "      <td>2533</td>\n",
       "      <td>102</td>\n",
       "      <td>42</td>\n",
       "      <td>topic</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-04-07 16:46:14</td>\n",
       "      <td>2023-04-07 16:46:27</td>\n",
       "      <td>2023-04-07 16:46:27</td>\n",
       "      <td>math</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2023-04-07</td>\n",
       "      <td>0 days 00:00:13</td>\n",
       "      <td>0.216667</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50137</th>\n",
       "      <td>114742</td>\n",
       "      <td>955</td>\n",
       "      <td>647</td>\n",
       "      <td>42</td>\n",
       "      <td>topic</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-03-07 06:36:32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-03-07 06:36:32</td>\n",
       "      <td>math</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2025-03-07</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50138</th>\n",
       "      <td>114743</td>\n",
       "      <td>955</td>\n",
       "      <td>106</td>\n",
       "      <td>42</td>\n",
       "      <td>topic</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-03-07 06:37:49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-03-07 06:37:49</td>\n",
       "      <td>math</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2025-03-07</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50139</th>\n",
       "      <td>114744</td>\n",
       "      <td>955</td>\n",
       "      <td>104</td>\n",
       "      <td>42</td>\n",
       "      <td>topic</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-03-07 06:37:50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-03-07 06:37:50</td>\n",
       "      <td>math</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2025-03-07</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50140</th>\n",
       "      <td>114745</td>\n",
       "      <td>955</td>\n",
       "      <td>108</td>\n",
       "      <td>42</td>\n",
       "      <td>topic</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-03-07 06:37:54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-03-07 06:37:54</td>\n",
       "      <td>math</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2025-03-07</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50141</th>\n",
       "      <td>114746</td>\n",
       "      <td>955</td>\n",
       "      <td>2945</td>\n",
       "      <td>42</td>\n",
       "      <td>quiz</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-03-07 06:42:09</td>\n",
       "      <td>2025-03-07 06:54:21</td>\n",
       "      <td>2025-03-07 06:54:21</td>\n",
       "      <td>math</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2025-03-07</td>\n",
       "      <td>0 days 00:12:12</td>\n",
       "      <td>12.200000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25938 rows × 16 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T15:31:33.896846Z",
     "start_time": "2025-04-24T15:31:30.837952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Rolling window for recent activity\n",
    "rolling_window_days = 10\n",
    "\n",
    "# Convert the 'date' columns to datetime\n",
    "activity_math['activity_updated'] = pd.to_datetime(activity_math['activity_updated'])\n",
    "performances_math['time'] = pd.to_datetime(performances_math['time'])\n",
    "\n",
    "def compute_all_features_for_exam_no_time(exam_row, user_activities, user_exams, window_days=rolling_window_days):\n",
    "\n",
    "    exam_dt = exam_row['time']\n",
    "\n",
    "    # Include activities up to and including exam_date\n",
    "    previous_activities = user_activities[user_activities['activity_updated'] < exam_dt].copy()\n",
    "\n",
    "    # Rolling window (activities in the last N days, including exam day)\n",
    "    window_start = exam_dt - pd.Timedelta(days=window_days)\n",
    "    rolling_activities = previous_activities[previous_activities['activity_updated'] >= window_start].copy()\n",
    "\n",
    "    features = {}\n",
    "\n",
    "    # Recent average time per activity (rolling window)\n",
    "    #total_time_rolling = rolling_activities['time_in_minutes'].sum()\n",
    "    count_rolling = len(rolling_activities)\n",
    "    #features['recent_avg_time_per_activity'] = total_time_rolling / count_rolling if count_rolling > 0 else 0\n",
    "\n",
    "    # Number of days since last activity\n",
    "    if not previous_activities.empty:\n",
    "        last_activity_date = previous_activities['activity_updated'].max()\n",
    "        features['days_since_last_activity'] = (exam_dt - last_activity_date).days\n",
    "    else:\n",
    "        features['days_since_last_activity'] = np.nan\n",
    "\n",
    "    # Total time spent on activities before the exam\n",
    "    #features['total_time_spent_on_activity_before_exam'] = previous_activities['time_in_minutes'].sum() if not previous_activities.empty else 0\n",
    "\n",
    "    # Average percentage on past exams\n",
    "    previous_exams = user_exams[user_exams['time'] < exam_dt]\n",
    "    features['average_performance_past_exams'] = previous_exams['performance'].mean() if not previous_exams.empty else np.nan\n",
    "\n",
    "    # Usage Frequency: Average activities per day in rolling window & Active days ratio\n",
    "    features['avg_activities_per_day_recent'] = count_rolling / window_days if window_days > 0 else np.nan\n",
    "    if not rolling_activities.empty:\n",
    "        distinct_days = rolling_activities['activity_updated'].dt.normalize().nunique()\n",
    "    else:\n",
    "        distinct_days = 0\n",
    "    features['active_days_ratio_recent'] = distinct_days / window_days if window_days > 0 else np.nan\n",
    "\n",
    "    # Activity diversity (rolling window)\n",
    "    features['diversity_recent'] = rolling_activities['activity_type'].nunique() if not rolling_activities.empty else 0\n",
    "\n",
    "\n",
    "    return pd.Series(features)\n",
    "\n",
    "# Loop over each exam (grouped by user) in performances_math and compute all features.\n",
    "features_list = []\n",
    "\n",
    "for user_id, user_exams in performances_math.groupby('user_id'):\n",
    "    # Get corresponding activities for the user from activity_math and sort by date\n",
    "    user_activities = activity_math[activity_math['user_id'] == user_id].sort_values('activity_updated')\n",
    "    user_exams_sorted = user_exams.sort_values('time')\n",
    "\n",
    "    for exam_index, exam_row in user_exams_sorted.iterrows():\n",
    "        feats = compute_all_features_for_exam_no_time(exam_row, user_activities, user_exams_sorted, rolling_window_days)\n",
    "        feats['exam_index'] = exam_index\n",
    "        features_list.append(feats)\n",
    "\n",
    "# Output df\n",
    "features_df = pd.DataFrame(features_list).set_index('exam_index')\n",
    "performances_math_features = performances_math.join(features_df, how='left')"
   ],
   "id": "af18bda9a1342ebd",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T17:00:01.501201Z",
     "start_time": "2025-04-24T17:00:01.489082Z"
    }
   },
   "cell_type": "code",
   "source": "print(performances_math_features.head(5))",
   "id": "7e694495d6fa7e12",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      user_id domain test_id  course        date                time  \\\n",
      "9.0         6   math      42    3865  2024-11-23 2024-11-23 10:25:34   \n",
      "10.0        6   math      48    3865  2025-01-08 2025-01-08 14:48:04   \n",
      "11.0        6   math      49    3865  2025-01-08 2025-01-08 15:29:07   \n",
      "12.0        6   math      50    3865  2025-02-04 2025-02-04 15:36:38   \n",
      "13.0        6   math      54    3865  2024-11-23 2024-11-23 11:26:10   \n",
      "\n",
      "      percentage  performance  recent_avg_time_per_activity  \\\n",
      "9.0        25.00       -36.04                     19.300000   \n",
      "10.0       50.00        -1.92                     27.858333   \n",
      "11.0       66.67        21.23                     27.700000   \n",
      "12.0       54.55        19.57                      0.000000   \n",
      "13.0       14.29       -47.71                      8.838095   \n",
      "\n",
      "      days_since_last_activity  total_time_spent_on_activity_before_exam  \\\n",
      "9.0                        7.0                                 43.016667   \n",
      "10.0                       0.0                                410.966667   \n",
      "11.0                       0.0                                438.033333   \n",
      "12.0                      18.0                                508.283333   \n",
      "13.0                       0.0                                 66.283333   \n",
      "\n",
      "      average_performance_past_exams  avg_activities_per_day_recent  \\\n",
      "9.0                              NaN                            0.2   \n",
      "10.0                        -23.8025                            0.4   \n",
      "11.0                        -19.4260                            0.5   \n",
      "12.0                        -12.6500                            0.0   \n",
      "13.0                        -36.0400                            0.7   \n",
      "\n",
      "      active_days_ratio_recent  diversity_recent  \n",
      "9.0                        0.1               1.0  \n",
      "10.0                       0.2               2.0  \n",
      "11.0                       0.2               2.0  \n",
      "12.0                       0.0               0.0  \n",
      "13.0                       0.2               2.0  \n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T15:31:35.489409Z",
     "start_time": "2025-04-24T15:31:35.481744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# scaling the columns\n",
    "columns_to_scale = [ 'days_since_last_activity','average_performance_past_exams','avg_activities_per_day_recent','active_days_ratio_recent','diversity_recent']\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_values = scaler.fit_transform(performances_math_features[columns_to_scale])\n",
    "scaled_df = pd.DataFrame(scaled_values, columns=columns_to_scale, index=performances_math_features.index)\n",
    "remaining_df = performances_math_features.drop(columns=columns_to_scale)\n",
    "final_df = pd.concat([scaled_df, remaining_df], axis=1)"
   ],
   "id": "8d4180885034dfa1",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T15:31:36.303975Z",
     "start_time": "2025-04-24T15:31:36.298064Z"
    }
   },
   "cell_type": "code",
   "source": "final_df.dropna(inplace=True)",
   "id": "85cbd1959b464068",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T15:31:37.665858Z",
     "start_time": "2025-04-24T15:31:37.647418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Linear Regression Model\n",
    "mod = smf.ols(\n",
    "    formula='performance ~  days_since_last_activity + average_performance_past_exams + avg_activities_per_day_recent + active_days_ratio_recent + diversity_recent',\n",
    "    data=final_df)\n",
    "\n",
    "# Fit the model\n",
    "res = mod.fit()\n",
    "\n",
    "# Print regression results summary\n",
    "print(res.summary())"
   ],
   "id": "1372a3ad1a833771",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:            performance   R-squared:                       0.259\n",
      "Model:                            OLS   Adj. R-squared:                  0.258\n",
      "Method:                 Least Squares   F-statistic:                     233.1\n",
      "Date:                Thu, 24 Apr 2025   Prob (F-statistic):          5.80e-214\n",
      "Time:                        17:31:37   Log-Likelihood:                -15444.\n",
      "No. Observations:                3341   AIC:                         3.090e+04\n",
      "Df Residuals:                    3335   BIC:                         3.094e+04\n",
      "Df Model:                           5                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==================================================================================================\n",
      "                                     coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Intercept                          0.3074      0.428      0.719      0.472      -0.531       1.146\n",
      "days_since_last_activity           1.4564      0.544      2.677      0.007       0.390       2.523\n",
      "average_performance_past_exams    14.0834      0.439     32.099      0.000      13.223      14.944\n",
      "avg_activities_per_day_recent     -1.6276      0.534     -3.045      0.002      -2.675      -0.580\n",
      "active_days_ratio_recent           1.4006      0.554      2.527      0.012       0.314       2.487\n",
      "diversity_recent                   0.5440      0.603      0.902      0.367      -0.639       1.727\n",
      "==============================================================================\n",
      "Omnibus:                       26.139   Durbin-Watson:                   1.660\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               18.000\n",
      "Skew:                          -0.031   Prob(JB):                     0.000123\n",
      "Kurtosis:                       2.646   Cond. No.                         2.61\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "If we try to keep all the data we loose two important features regarding the time spent on activity and finish with a model that is less good than the original one (R^2 of 0.259 vs 0.267). DEVELOPP A BIT MORE",
   "id": "70e292973cf6701c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
